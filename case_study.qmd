---
title: "Case Study - STM"
format: html
editor: visual
---

# Case Study 1: Biomass

*Data and ideas for this case study come from (Goicoa et al., 2011).*

To estimate the amount of carbon dioxide retained in a tree, its biomass needs to be known and multiplied by an expansion factor (there are several alternatives in the literature). To calculate the biomass, specific regression equations by species are frequently used. These regression equations, called allometric equations, estimate the biomass of the tree by means of some known characteristics, typically diameter and/or height of the stem and branches. The BIOMASS file contains data of 42 beeches (Fagus Sylvatica) from a forest of Navarra (Spain) in 2006, where

• `diameter`: diameter of the stem in centimeters

• `height`: height of the tree in meters

• `stemweight`: weight of the stem in kilograms

• `aboveweight`: aboveground weight in kilograms

```{r}
library(PASWR2)
data(BIOMASS)
head(BIOMASS)
```

#### (a) Create a scatterplot of aboveweight versus diameter. Is the relationship linear? Superimpose a regression line over the plot just created.

```{r}
library(ggplot2)
library(car)

# Create scatterplot with regression line
plot_a <- ggplot(BIOMASS, aes(x = diameter, y = aboveweight)) +
  geom_point(size = 3, color = "darkblue", alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "red", linewidth = 1) +
  labs(title = "Aboveground Weight vs Diameter",
       x = "Diameter (cm)",
       y = "Aboveground Weight (kg)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

print(plot_a)
```

**No, the relationship is NOT linear.**

**Observations:**

1.  **Curved Pattern**: The scatterplot shows a clear **curvilinear (non-linear) relationship** between diameter and aboveground weight. The data points follow a curved pattern rather than a straight line.

2.  **Accelerating Growth**: The relationship appears to be **exponential or power-law** in nature. As diameter increases, the aboveground weight increases at an accelerating rate (the curve becomes steeper).

3.  **Poor Linear Fit**: While the red regression line has been superimposed on the plot, you can see that:

    -   At small diameters (left side), the line overestimates the weight

    -   In the middle range, the fit is reasonable

    -   At large diameters (right side), the line underestimates the weight

    -   This systematic pattern of residuals (points above and below the line in a curved pattern) indicates that a linear model is **not appropriate** for this data.

4.  **Biological Interpretation**: This makes biological sense because tree biomass is related to volume, which grows as a function of diameter raised to a power (typically around 2-3), not linearly.

#### (b) Create a scatterplot of $\log(aboveweight)$ versus $\log(diameter)$. Is the relationship linear? Superimpose a regression line over the plot just created.

```{r}
# Create log-transformed scatterplot
plot_b <- ggplot(BIOMASS, aes(x = log(diameter), y = log(aboveweight))) +
  geom_point(size = 3, color = "darkgreen", alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "red", linewidth = 1) +
  labs(title = "Log(Aboveground Weight) vs Log(Diameter)",
       x = "Log(Diameter)",
       y = "Log(Aboveground Weight)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

print(plot_b)
```

**Yes, the relationship is linear.**

**Observations:**

1.  **Strong Linear Pattern**: After the log transformation, the data points now follow a clear **straight-line pattern**. The points are distributed closely around the red regression line with no systematic curvature.

2.  **Excellent Linear Fit**: The regression line fits the data much better than in part (a):

    -   Points are randomly scattered above and below the line

    -   No systematic pattern of over/under-estimation across the range

    -   The residuals appear to have constant variance (homoscedastic)

    -   Very tight clustering around the line suggests high R²

3.  **Success of Log Transformation**: The log-log transformation has successfully **linearized** the power-law relationship that was evident in part (a). This transformation is standard for allometric equations.

4.  **Mathematical Interpretation**: The linear relationship in log-log space indicates that the original relationship follows a power law: $\log(aboveweight) = β_0 + β_1 \log(diameter)$ Which corresponds to the power relationship: $aboveweight = \exp(β_0) × diameter^{β_1}$

5.  **Allometric Equation**: This is exactly the form expected for allometric equations in biology, where biomass scales as a power function of dimensional measurements like diameter.

#### (c) Fit the regression model $\log(aboveweight) = β_0 + β_1\log(diameter)$, and compute $R^2, R^2_a$ and the variance of the residuals.

```{r}
# Fit the model
model_c <- lm(log(aboveweight) ~ log(diameter), data = BIOMASS)

# Display results
cat("\nModel Summary:\n")
print(summary(model_c))

# Extract R-squared values
r_squared_c <- summary(model_c)$r.squared
r_squared_adj_c <- summary(model_c)$adj.r.squared
residual_var_c <- summary(model_c)$sigma^2

cat("\n--- Model Statistics ---\n")
cat(sprintf("R-squared: %.4f\n", r_squared_c))
cat(sprintf("Adjusted R-squared: %.4f\n", r_squared_adj_c))
cat(sprintf("Residual variance (σ²): %.4f\n", residual_var_c))
```

**Model Statistics:**

**1. R-squared (R²) = 0.9779**

**Interpretation:** The model explains **97.79% of the variance** in log(aboveground weight). This indicates an excellent fit - nearly all the variation in log-transformed biomass can be explained by log-transformed diameter alone.

**2. Adjusted R-squared (R²ₐ) = 0.9774**

**Interpretation:** After adjusting for the number of predictors (1 predictor), the model still explains **97.74%** of the variance. The minimal difference between R² and R²ₐ (0.0005) indicates that the model is not overfitted and the predictor is genuinely useful.

**3. Residual Variance (σ²) = 0.0339**

**Interpretation:** The variance of the residuals around the regression line is **0.0339** (in log units). The residual standard error is σ = √0.0339 = **0.1842**, which represents the typical prediction error on the log scale.

#### (d) Introduce $log(height)$ as an explanatory variable and fit the model$\log(aboveweight) =β_0 + β_1 \log(diameter) + β_2 \log(height)$. What is the effect of introducing log(height)in the model?

```{r}
# Fit the multiple regression model
model_d <- lm(log(aboveweight) ~ log(diameter) + log(height), data = BIOMASS)

# Display results
cat("\nModel Summary:\n")
print(summary(model_d))

# Extract R-squared values
r_squared_d <- summary(model_d)$r.squared
r_squared_adj_d <- summary(model_d)$adj.r.squared
residual_var_d <- summary(model_d)$sigma^2

cat("\n--- Effect of Adding log(height) ---\n")
cat(sprintf("Change in R²: %.4f → %.4f (Δ = %.4f)\n", 
            r_squared_c, r_squared_d, r_squared_d - r_squared_c))
cat(sprintf("Change in Adjusted R²: %.4f → %.4f (Δ = %.4f)\n", 
            r_squared_adj_c, r_squared_adj_d, r_squared_adj_d - r_squared_adj_c))
cat(sprintf("Change in Residual Variance: %.4f → %.4f (Δ = %.4f)\n", 
            residual_var_c, residual_var_d, residual_var_d - residual_var_c))
```

**Key Improvements:**

**Better Explanatory Power**

-   R² increased from **97.79% to 98.56%**

-   The model now explains an **additional 0.77%** of the variance in log(aboveground weight)

-   While this seems small, it represents meaningful improvement in an already excellent model

**Improved Adjusted R²**

-   Adjusted R² increased from **97.74% to 98.49%** (+0.75%)

-   The positive change in R²ₐ confirms that adding height improves the model even after penalizing for the additional parameter

-   This indicates height is a **genuinely useful predictor**, not just noise

**Reduced Residual Variance**

-   Residual variance decreased by **33.3%**: from 0.0339 to 0.0226

-   Residual standard error reduced by **18.3%**: from 0.1842 to 0.1505

-   **Predictions are now more precise** with smaller typical errors

**Height is Statistically Significant**

-   **β₂ (log height) = 0.5292**

-   **p-value = 4.71 × 10⁻⁵** (highly significant)

-   **t-value = 4.577** (strong evidence)

-   Height contributes significantly to predicting biomass beyond diameter alone

**Interpretation of the Height Effect:**

**Elasticity Interpretation:**

**β₂ = 0.5292** means:

-   A **1% increase in height** is associated with approximately a **0.53% increase in aboveground weight**, holding diameter constant

-   This is additional to the diameter effect (β₁ = 2.178)

**Biological Meaning:**

-   **Diameter captures most of the biomass** (β₁ = 2.178, the dominant effect)

-   **Height adds important information** about tree form and structure

-   Trees with greater height for a given diameter have proportionally more biomass

-   This accounts for variation in tree shape, branching patterns, and crown development

#### (e) Complete the Analysis questions for the model in (d).

#### Analysis questions:

##### (1) Estimate the model’s parameters and their standard errors. Provide an interpretation for the model’s parameters.

```{r}
coef_summary <- summary(model_d)$coefficients
print(coef_summary)

cat("\nInterpretation:\n")
cat(sprintf("β₀ (Intercept) = %.4f: Log of expected aboveground weight when\n", 
            coef(model_d)[1]))
cat("                        log(diameter) and log(height) are zero.\n")
cat(sprintf("β₁ (log diameter) = %.4f: A 1%% increase in diameter is associated with\n", 
            coef(model_d)[2]))
cat(sprintf("                          approximately a %.2f%% increase in aboveground weight,\n", 
            coef(model_d)[2]))
cat("                          holding height constant.\n")
cat(sprintf("β₂ (log height) = %.4f: A 1%% increase in height is associated with\n", 
            coef(model_d)[3]))
cat(sprintf("                        approximately a %.2f%% increase in aboveground weight,\n", 
            coef(model_d)[3]))
cat("                        holding diameter constant.\n")
```

**Detailed Interpretations:**

**1. β₀ (Intercept) = -2.7771**

**Statistical Significance:** Highly significant (p = 1.18 × 10⁻¹⁰)

**Interpretation:**

-   This is the expected value of log(aboveweight) when both log(diameter) = 0 and log(height) = 0

-   In other words, when diameter = 1 cm and height = 1 m

-   On the original scale: exp(-2.7771) ≈ **0.062 kg**

-   However, this is an **extrapolation** beyond the data range and should not be interpreted literally, as trees with 1 cm diameter and 1 m height are not meaningful in this dataset

**2. β₁ (log diameter) = 2.1778**

**Statistical Significance:** Extremely significant (p = 8.32 × 10⁻³⁵, t = 43.867)

**Interpretation (Elasticity):**

-   **β₁ = 2.1778** represents the **elasticity of aboveground weight with respect to diameter**

-   A **1% increase in diameter** is associated with approximately a **2.18% increase in aboveground weight**, holding height constant

-   Equivalently: **aboveweight ∝ diameter\^2.18** (when height is held constant)

**Biological Meaning:**

-   Diameter has a **strong positive effect** on biomass

-   The exponent \> 2 indicates that biomass increases **more than proportionally** with diameter

-   This reflects the fact that biomass depends on both cross-sectional area (∝ diameter²) and additional structural components

-   The exponent of \~2.18 is consistent with theoretical and empirical allometric relationships for trees

**3. β₂ (log height) = 0.5292**

**Statistical Significance:** Highly significant (p = 4.71 × 10⁻⁵, t = 4.577)

**Interpretation (Elasticity):**

-   **β₂ = 0.5292** represents the **elasticity of aboveground weight with respect to height**

-   A **1% increase in height** is associated with approximately a **0.53% increase in aboveground weight**, holding diameter constant

-   Equivalently: **aboveweight ∝ height\^0.53** (when diameter is held constant)

**Biological Meaning:**

-   Height has a **moderate positive effect** on biomass beyond what diameter captures

-   For trees with the same diameter, taller trees have more aboveground biomass

-   This captures variation in:

    -   **Crown size and branch development**

    -   **Tree form** (slender vs. stocky trees)

    -   **Vertical biomass distribution**

-   The exponent \~0.5 suggests height contributes roughly as the square root, which is less than proportional

##### (2) Compute the variance-covariance matrix of the $\hat{\beta}_s$.

```{r}
vcov_matrix <- vcov(model_d)
print(vcov_matrix)
```

**Interpretation of Matrix Elements:**

**1. Diagonal Elements (Variances):**

**Interpretation:**

-   **Var(β̂₀) = 0.1022:** The intercept has the **highest variance**, indicating the most uncertainty in its estimate

-   **Var(β̂₁) = 0.0025:** The diameter coefficient has **very low variance**, indicating high precision (this is our most precisely estimated parameter)

-   **Var(β̂₂) = 0.0134:** The height coefficient has **moderate variance**, more uncertain than diameter but still reasonably precise

**2. Off-Diagonal Elements (Covariances):**

The off-diagonal elements represent the **covariances** between pairs of parameter estimates:

**Cov(β̂₀, β̂₁) = -0.00061**

-   **Negative covariance** between intercept and log(diameter)

-   Very small magnitude → weak relationship

-   If β̂₁ is higher than expected, β̂₀ tends to be slightly lower

**Cov(β̂₀, β̂₂) = -0.03222**

-   **Negative covariance** between intercept and log(height)

-   Moderate magnitude → noticeable relationship

-   If β̂₂ is higher than expected, β̂₀ tends to be lower

-   This is the **strongest covariance** in the matrix

**Cov(β̂₁, β̂₂) = -0.00260**

-   **Negative covariance** between log(diameter) and log(height)

-   Small magnitude → weak relationship

-   If β̂₁ is higher than expected, β̂₂ tends to be slightly lower

##### (3) Provide 95% confidence intervals for $\beta_1$ and $\beta_2$.

```{r}
conf_int <- confint(model_d, level = 0.95)
print(conf_int)

cat("\nInterpretation:\n")
cat(sprintf("β₁: We are 95%% confident that the true elasticity of aboveground weight\n"))
cat(sprintf("    with respect to diameter is between %.4f and %.4f.\n", 
            conf_int[2,1], conf_int[2,2]))
cat(sprintf("β₂: We are 95%% confident that the true elasticity of aboveground weight\n"))
cat(sprintf("    with respect to height is between %.4f and %.4f.\n", 
            conf_int[3,1], conf_int[3,2]))
```

**Interpretation of β₁ (log diameter):** **95% CI: \[2.0774, 2.2782\]**

**Statistical Interpretation:**

-   We are **95% confident** that the true population parameter β₁ lies between 2.0774 and 2.2782

-   If we repeated this study many times, approximately 95% of such intervals would contain the true β₁

**Elasticity Interpretation:**

-   We are 95% confident that a **1% increase in diameter** is associated with between a **2.08% and 2.28% increase in aboveground weight**, holding height constant

**Practical Implications:**

1.  **Highly Precise Estimate:**

    -   Interval width = 0.2008 (very narrow)

    -   Relative to the estimate (2.1778), this is only ±4.6% variation

    -   The small standard error (0.0496) results in a tight confidence interval

    -   This reflects the **strong relationship** between diameter and biomass

2.  **Significantly Different from Zero:**

    -   The entire interval is **far above zero** (minimum = 2.0774)

    -   Strong evidence that diameter has a **positive effect** on biomass

    -   p-value \< 2.2 × 10⁻¹⁶ confirms this is extremely significant

3.  **Biological Interpretation:**

    -   The interval (2.08 - 2.28) confirms biomass scales **more than quadratically** with diameter

    -   Consistent with theoretical expectations: biomass depends on volume (∝ diameter²) plus bark, branches, and structural components

    -   The exponent being definitively \> 2 (lower bound = 2.0774) is biologically meaningful

4.  **Comparison to Theory:**

    -   Simple volume would suggest β₁ = 2 (area of cross-section)

    -   Our interval \[2.08, 2.28\] exceeds 2, indicating **additional scaling effects**

    -   This is typical for tree allometry

##### (4) Compute the $R^2, R^2_a$ and the residual variance.

```{r}
cat(sprintf("R-squared (R²): %.4f\n", r_squared_d))
cat(sprintf("  → The model explains %.2f%% of the variance in log(aboveground weight)\n", 
            r_squared_d * 100))
cat(sprintf("Adjusted R-squared (R²ₐ): %.4f\n", r_squared_adj_d))
cat(sprintf("  → Adjusts for number of predictors\n"))
cat(sprintf("Residual variance (σ²): %.4f\n", residual_var_d))
cat(sprintf("Residual standard error (σ): %.4f\n", sqrt(residual_var_d)))
```

**1. R-squared (R²) = 0.9856**

**Definition:**

R² measures the **proportion of variance in the dependent variable** (log(aboveground weight)) that is **explained by the independent variables** (log(diameter) and log(height)).

**Interpretation:**

**Statistical Interpretation:**

-   The model explains **98.56% of the total variance** in log(aboveground weight)

-   Only **1.44% of the variance remains unexplained** (due to random error and unmeasured factors)

**Practical Interpretation:**

-   This is an **excellent fit** - nearly all variation in tree biomass is captured by diameter and height

-   The model has **very high predictive power**

-   The allometric relationship is **extremely strong and consistent**

**Quality Assessment:**

-   R² \> 0.95 is considered **excellent** in biological sciences

-   R² = 0.9856 indicates the model is **highly effective** at predicting biomass

-   Very little room for improvement with additional predictors

**2. Adjusted R-squared (R²ₐ) = 0.9849**

**Definition:**

Adjusted R² modifies R² to account for the **number of predictors** in the model, penalizing for adding variables that don't substantially improve fit.

**Interpretation:**

**Statistical Interpretation:**

-   After adjusting for the 2 predictors, the model still explains **98.49% of the variance**

-   The adjustment penalty is minimal: R² - R²ₐ = 0.9856 - 0.9849 = **0.0007** (0.07%)

**Why the Small Difference?**

-   **Large sample size** relative to predictors (n=42, p=2)

-   Both predictors are **highly significant** and contribute meaningfully

-   No "overfitting" - the predictors genuinely improve the model

**Model Selection Implications:**

-   The small difference (0.07%) indicates **both predictors are valuable**

-   If we had added an irrelevant predictor, R²ₐ would have decreased while R² increased

-   The fact that R²ₐ is nearly identical to R² confirms our model is **parsimonious and well-specified**

**3. Residual Variance (σ²) = 0.0226**

**Definition:**

The residual variance measures the **average squared deviation** of observations from the fitted regression line on the log scale.

**Interpretation:**

**Statistical Interpretation:**

-   The variance of the residuals around the regression line is **0.0226** (in log units)

-   This represents the **unexplained variation** in the model

-   Lower values indicate better fit

**Scale Consideration:**

-   This is on the **log scale**, so it's not directly interpretable in kg

-   However, 0.0226 is **quite small** for log-transformed data

-   It indicates **tight clustering** of observations around the fitted line

**Comparison to Simple Model:**

-   Simple model (diameter only): σ² = 0.0339

-   Multiple model (diameter + height): σ² = 0.0226

-   Reduction: 0.0339 - 0.0226 = 0.0113

-   **33.3% reduction** in residual variance by adding height

**Quality Assessment:**

-   Small residual variance indicates **high precision**

-   Predictions will have **narrow prediction intervals**

-   The model captures the systematic relationship very well

**4. Residual Standard Error (σ) = 0.1505**

**Definition:**

The residual standard error is the **square root of the residual variance**, representing the typical size of a residual.

**Interpretation:**

**Statistical Interpretation:**

-   The **typical prediction error** is about 0.1505 on the log scale

-   This is the standard deviation of the residuals

-   About 68% of observations fall within ±0.1505 of their predicted log(weight)

-   About 95% fall within ±2(0.1505) = ±0.301 of their predicted log(weight)

**Back-transformation to Original Scale:** To understand the prediction error in terms of actual weight:

-   A residual of +0.1505 on log scale → multiplicative error of exp(0.1505) ≈ **1.162** (16.2% overestimation)

-   A residual of -0.1505 on log scale → multiplicative error of exp(-0.1505) ≈ **0.860** (14.0% underestimation)

**Practical Interpretation:**

-   Typical predictions are accurate within about **±14-16%** on the original scale

-   This is **excellent precision** for biological/ecological models

-   For carbon accounting, this level of accuracy is highly acceptable

**Comparison to Simple Model:**

-   Simple model: σ = 0.1842

-   Multiple model: σ = 0.1505

-   Improvement: **18.3% reduction** in typical prediction error

##### (5) Construct a graph with the default diagnostics plots of R.

```{r}
par(mfrow = c(2, 2))
plot(model_d, which = 1:4)
par(mfrow = c(1, 1))
```

**1. Residuals vs Fitted (Top Left)**

**Purpose:**

Checks for **linearity** and **homoscedasticity** (constant variance).

**What to Look For:**

-   Residuals should be randomly scattered around zero

-   No systematic patterns (curves, funnels, or trends)

-   Horizontal red line near zero

**Observations:**

**GOOD SIGNS:**

-   Residuals are **randomly scattered** around the horizontal zero line

-   The red smoothed line is **nearly horizontal** and close to zero

-   No clear **curved pattern** (U-shape or inverted U-shape)

-   Points are distributed relatively evenly above and below zero

**MINOR CONCERNS:**

-   There's a **slight suggestion of clustering** around fitted values 7-8

-   A few labeled points (10, 11, 15) show slightly larger residuals

-   **Very slight hint** of increasing spread at higher fitted values (but minimal)

**Conclusion:**

The **linearity assumption is satisfied** - the log-log transformation successfully linearized the relationship. **Homoscedasticity appears reasonable** with only very minor heteroscedasticity, if any.

**2. Normal Q-Q Plot (Top Right)**

**Purpose:**

Checks if residuals follow a **normal distribution**.

**What to Look For:**

-   Points should fall along the diagonal dashed line

-   No systematic deviations (S-curves, heavy tails, or outliers)

**Observations:**

**EXCELLENT:**

-   The vast majority of points **follow the diagonal line closely**

-   The middle portion (between quantiles -1 to +1.5) shows **excellent alignment**

-   This represents about 80-90% of the data

**SLIGHT DEVIATIONS:**

-   **Right tail (upper quantiles):** Points 11, 10, 9, 15 slightly deviate upward

    -   These observations have **slightly larger positive residuals** than expected under perfect normality

    -   Suggests very mild **right skewness** or potential outliers

-   **Left tail (lower quantiles):** Minor deviation in the lower left

    -   Less pronounced than right tail

**Conclusion:**

The **normality assumption is largely satisfied**. The deviations are **minor** and unlikely to seriously affect inference. The slight right-tail deviation is common in biological data and not severe enough to warrant concern for a sample of n=42.

**3. Scale-Location Plot (Bottom Left)**

**Purpose:**

Another check for **homoscedasticity** (constant variance) using standardized residuals.

**What to Look For:**

-   Random scatter of points

-   Horizontal red line

-   Constant vertical spread across fitted values

**Observations:**

**GOOD SIGNS:**

-   The red smoothed line is **relatively flat** (slight upward trend but minimal)

-   Points show **reasonably consistent spread** across the range of fitted values

-   Most points cluster between √\|standardized residuals\| of 0.5 and 1.2

**MINOR CONCERNS:**

-   Again, observations 10, 11, and 15 stand out with higher values

-   **Very slight upward trend** in the red line at higher fitted values

    -   Suggests marginally increasing variance with predicted weight

    -   However, the trend is **very modest**

**Conclusion:**

**Homoscedasticity assumption is reasonably satisfied**. There's a very slight hint of increasing variance at higher fitted values, but it's minor and unlikely to seriously violate the assumption. The constant variance assumption holds **reasonably well**.

**4. Residuals vs Leverage (Cook's Distance) (Bottom Right)**

**Purpose:**

Identifies **influential observations** that have high leverage and/or large residuals.

**What to Look For:**

-   Points outside Cook's distance contours (dashed red lines, typically 0.5 or 1.0)

-   Points in upper-right or lower-right corners (high leverage + large residual)

-   Most points should have low Cook's distance

**Observations:**

**EXCELLENT:**

-   **No points exceed Cook's distance of 0.5** (no dashed red contour lines are visible)

-   All observations have **Cook's distance well below 0.2**

-   Most points cluster near **Cook's distance \< 0.1**

**Notable Observations:**

-   **Observation 11:** Highest Cook's distance (\~0.15) but still well below concerning threshold

-   **Observation 15:** Also elevated (\~0.13)

-   **Observation 13:** Moderate Cook's distance (\~0.12)

-   These points have **some influence** but are **not problematic**

**Leverage Assessment:**

-   All points have **relatively low leverage** (x-axis values \< 0.15)

-   No extreme leverage points that would dominate the regression

-   The maximum leverage is well below concerning thresholds (typically 2p/n = 6/42 = 0.14)

**Conclusion:**

**No influential observations** of serious concern. While observations 11, 13, and 15 have slightly elevated Cook's distances, they remain well within acceptable ranges and do not unduly influence the regression results.

##### (6) Can homogeneity of variance be assumed?

```{r}
# Breusch-Pagan test
library(lmtest)
bp_test <- lmtest::bptest(model_d)
cat("\nBreusch-Pagan Test for Heteroscedasticity:\n")
print(bp_test)

if(bp_test$p.value > 0.05) {
  cat("\nConclusion: Fail to reject H₀ (p > 0.05).")
  cat("\nHomogeneity of variance can be assumed.\n")
} else {
  cat("\nConclusion: Reject H₀ (p < 0.05).")
  cat("\nEvidence of heteroscedasticity (non-constant variance).\n")
}

# Visual assessment
cat("\nVisual Assessment: Check the Residuals vs Fitted plot.")
cat("\nIf residuals are randomly scattered around zero with constant spread,")
cat("\nhomoscedasticity is reasonable.\n")
```

**Statistical Test Results:**

**Breusch-Pagan Test for Heteroscedasticity:**

| Test Statistic | df  | p-value | Decision  |
|----------------|-----|---------|-----------|
| BP = 7.4101    | 2   | 0.0246  | Reject H₀ |

**Hypotheses:**

-   **H₀:** Homoscedasticity (constant variance of residuals)

-   **H₁:** Heteroscedasticity (non-constant variance of residuals)

**Test Interpretation:**

-   **p-value = 0.0246 \< 0.05** → Reject the null hypothesis at α = 0.05

-   **Statistical conclusion:** There is evidence of **heteroscedasticity**

**Answer: Homogeneity of Variance - BORDERLINE CASE**

**TECHNICAL ANSWER: Marginally Violated**

Based on the **formal statistical test alone**, we would conclude:

-   **No, homogeneity of variance cannot be fully assumed**

-   The Breusch-Pagan test detects **statistically significant heteroscedasticity** (p = 0.0246)

**Technically:**

**No, perfect homogeneity of variance cannot be assumed** - the Breusch-Pagan test detects statistically significant heteroscedasticity (p = 0.0246).

**Practically:**

**The violation is MILD and does NOT seriously compromise the model**:

-   It's a **borderline case** (p = 0.0246, just below 0.05)

-   Visual diagnostics show only **minor variance changes**

-   Parameter estimates remain **valid and unbiased**

-   Inference remains **reliable** given the extremely strong statistical significance of predictors

-   Predictions remain **highly accurate** (R² = 0.9856)

##### (7) Do the residuals appear to follow a normal distribution?

```{r}
# Shapiro-Wilk test
shapiro_test <- shapiro.test(residuals(model_d))
cat("\nShapiro-Wilk Normality Test:\n")
print(shapiro_test)

if(shapiro_test$p.value > 0.05) {
  cat("\nConclusion: Fail to reject H₀ (p > 0.05).")
  cat("\nResiduals appear to follow a normal distribution.\n")
} else {
  cat("\nConclusion: Reject H₀ (p < 0.05).")
  cat("\nResiduals may deviate from normality.\n")
}

# Q-Q plot
cat("\nVisual Assessment: Check the Normal Q-Q plot.")
cat("\nPoints should follow the diagonal line closely.\n")
```

**Statistical Test Results:**

**Shapiro-Wilk Normality Test:**

| Test Statistic | p-value | Decision          | α level |
|----------------|---------|-------------------|---------|
| W = 0.95133    | 0.07208 | Fail to reject H₀ | 0.05    |

**Hypotheses:**

-   **H₀:** The residuals follow a normal distribution

-   **H₁:** The residuals do NOT follow a normal distribution

**Test Interpretation:**

-   **p-value = 0.07208 \> 0.05** → Fail to reject the null hypothesis at α = 0.05

-   **Statistical conclusion:** There is **insufficient evidence to reject normality**

-   The residuals are **consistent with a normal distribution**

**Answer: YES, Residuals Appear to Follow a Normal Distribution**

**Evidence:**

1.  **Shapiro-Wilk test:** p = 0.072 \> 0.05 (fail to reject normality)

2.  **W statistic:** 0.951 (close to 1, indicates good fit)

3.  **Q-Q plot:** 80-90% of points follow diagonal closely

4.  **Minor tail deviations:** Present but not concerning

5.  **Overall assessment:** Normality assumption is **well satisfied**

**Conclusion:** The normality assumption **holds sufficiently well** for:

-   Valid statistical inference

-   Reliable confidence intervals

-   Trustworthy hypothesis tests

-   Accurate predictions

The minor deviations in the tails are **typical of real-world ecological data** and do **not undermine the model's validity**. The allometric regression model satisfies the normality assumption and can be **confidently used for inference and prediction**.

##### (8) Are there any outliers in the data?

```{r}
# Standardized residuals
std_resid <- rstandard(model_d)
outliers <- which(abs(std_resid) > 3)

cat(sprintf("\nObservations with |standardized residuals| > 3:\n"))
if(length(outliers) > 0) {
  cat("Potential outliers at observations:", outliers, "\n")
  print(data.frame(
    Observation = outliers,
    Std_Residual = std_resid[outliers]
  ))
} else {
  cat("No extreme outliers detected (using |std. residual| > 3 criterion).\n")
}

# Check for moderate outliers (> 2.5)
moderate_outliers <- which(abs(std_resid) > 2.5)
cat(sprintf("\nObservations with |standardized residuals| > 2.5:\n"))
if(length(moderate_outliers) > 0) {
  cat("Moderate outliers at observations:", moderate_outliers, "\n")
} else {
  cat("None found.\n")
}
```

**Criterion 1: Extreme Outliers (\|Standardized Residual\| \> 3)**

**Result:** **No extreme outliers detected**

**Interpretation:**

-   Using the stringent criterion of **\|standardized residual\| \> 3**

-   **Zero observations** exceed this threshold

-   This represents points more than **3 standard deviations** from the regression line

-   Expected under normality: \~0.3% of observations (0.13 out of 42)

-   **Observed: 0 observations** → No extreme outliers

**Criterion 2: Moderate Outliers (\|Standardized Residual\| \> 2.5)**

**Result:** **One moderate outlier detected**

| Observation | Status             |
|-------------|--------------------|
| **15**      | Moderate outlier ( |

**Interpretation:**

-   Using the moderate criterion of **\|standardized residual\| \> 2.5**

-   **One observation (#15)** exceeds this threshold

-   This represents a point more than **2.5 standard deviations** from the regression line

-   Expected under normality: \~1.2% of observations (0.50 out of 42)

-   **Observed: 1 observation (2.4%)** → Slightly above expected, but not alarming

**Answer: Minimal Outliers - Not a Concern**

**Summary:**

-   **Extreme outliers (\> 3σ):** None

-   **Moderate outliers (\> 2.5σ):** One (observation #15)

-   **Overall assessment:** The data has **very few outliers**, and those present are **not extreme**

##### (9) Are there any influential observations in the data?

```{r}
# Cook's distance
cooks_d <- cooks.distance(model_d)
influential <- which(cooks_d > 4/length(cooks_d))

cat("\nCook's Distance Analysis:\n")
cat(sprintf("Threshold: 4/n = 4/%d = %.4f\n", length(cooks_d), 4/length(cooks_d)))

if(length(influential) > 0) {
  cat("\nInfluential observations (Cook's D > 4/n):\n")
  print(data.frame(
    Observation = influential,
    Cooks_Distance = cooks_d[influential]
  ))
} else {
  cat("No highly influential observations detected.\n")
}

# DFBETAS
dfb <- dfbetas(model_d)
influential_dfb <- which(apply(abs(dfb), 1, max) > 2/sqrt(length(cooks_d)))

cat("\nDFBETAS Analysis:\n")
cat(sprintf("Threshold: 2/√n = 2/√%d = %.4f\n", 
            length(cooks_d), 2/sqrt(length(cooks_d))))

if(length(influential_dfb) > 0) {
  cat("\nInfluential observations (max |DFBETAS| > 2/√n):\n")
  cat(influential_dfb, "\n")
} else {
  cat("No influential observations detected using DFBETAS criterion.\n")
}

# Leverage
hat_values <- hatvalues(model_d)
high_leverage <- which(hat_values > 2 * 3 / length(hat_values))

cat("\nLeverage Analysis:\n")
cat(sprintf("Threshold: 2p/n = 2*3/%d = %.4f\n", 
            length(hat_values), 2 * 3 / length(hat_values)))

if(length(high_leverage) > 0) {
  cat("\nHigh leverage observations:\n")
  cat(high_leverage, "\n")
} else {
  cat("No high leverage points detected.\n")
}
```

**YES - but with important qualifications:**

**Mild to Moderate Influence Detected:**

-   **4 observations** (10, 11, 13, 15) show **mild to moderate influence** via Cook's Distance

-   **6 observations** (10, 11, 13, 14, 15, 39) influence **specific coefficients** via DFBETAS

-   **3 observations** (3, 34, 41) have **high leverage but fit well**

**None Are Severely Problematic:**

-   **Maximum Cook's D = 0.170** (well below concerning threshold of 0.5)

-   High leverage points **conform to the model** (small residuals)

-   Influence is **distributed** across multiple observations (no single dominant point)

**Model Remains Robust:**

-   All influential observations combined explain \< 10% of data

-   **Removing any/all would have minimal impact** on conclusions

-   Strong statistical significance (p \< 10⁻⁵) provides **large buffer**

-   R² = 0.9856 indicates **excellent fit even with these observations**

**Recommended Action:**

**RETAIN all observations** - they represent legitimate biological variation within acceptable influence bounds

**OPTIONAL: Verify data quality** for observations 13, 15, 11, 10 if original records available

**DOCUMENT** that influence diagnostics were checked and found acceptable

**PROCEED confidently** with inference and prediction using the full model

**Conclusion:**

While several observations show **detectable influence** on the regression, **none are severe enough to warrant removal or concern**. The model is **statistically robust**, all assumptions are **reasonably satisfied**, and the influential observations represent **natural biological variation** rather than data problems.

The allometric equation remains **valid, reliable, and appropriate for operational use** in estimating beech tree biomass for carbon accounting and forest inventory applications.

#### (f) Obtain predictions of the aboveground biomass of trees with diameters $diameter = seq(12.5, 42.5, 5)$ and heights $height = seq(10, 40, 5)$. Note that the weight predictions are obtained from back transforming the logarithm. The bias correction is obtained by means of the lognormal distribution: If $\hat{Y}_{pred}$ is the prediction, the corrected(back-transformed) prediction $\tilde{Y}_{pred}$ is given by

```{r}
# Create prediction data
pred_diameter <- seq(12.5, 42.5, 5)  
pred_height <- seq(10, 40, 5)        

# Create all combinations
pred_data <- expand.grid(
  diameter = pred_diameter,
  height = pred_height
)

cat(sprintf("\nCreating predictions for %d combinations of diameter and height\n", 
            nrow(pred_data)))

# Make predictions on log scale
pred_data$log_pred <- predict(model_d, 
                               newdata = data.frame(
                                 diameter = pred_data$diameter,
                                 height = pred_data$height
                               ))

# Extract residual variance (sigma squared)
sigma_sq <- summary(model_d)$sigma^2

# Back-transform with bias correction
pred_data$PSA_pred_naive <- exp(pred_data$log_pred)
pred_data$PSA_pred_corrected <- exp(pred_data$log_pred + sigma_sq/2)

cat("\nBias Correction Formula:\n")
cat(sprintf("ỹ_pred = exp(ŷ_pred + σ²/2)\n"))
cat(sprintf("where σ² = %.4f\n", sigma_sq))

cat("\n--- Sample Predictions ---\n")
print(head(pred_data[, c("diameter", "height", "PSA_pred_corrected")], 10))

cat("\n--- Full Prediction Table ---\n")
pred_table <- pred_data[, c("diameter", "height", 
                            "log_pred", "PSA_pred_corrected")]
colnames(pred_table) <- c("Diameter (cm)", "Height (m)", 
                          "Log(Weight)", "Predicted Weight (kg)")
print(head(pred_table))

# Create visualization of predictions
cat("\n\nGenerating prediction visualization...\n")

ggplot(pred_data, aes(x = diameter, y = PSA_pred_corrected, 
                      color = factor(height))) +
  geom_line(linewidth = 1) +
  geom_point(size = 3) +
  labs(title = "Predicted Aboveground Weight by Diameter and Height",
       x = "Diameter (cm)",
       y = "Predicted Aboveground Weight (kg)",
       color = "Height (m)") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        legend.position = "right")
```

**Key Patterns from Predictions:**

**1. Effect of Diameter (holding height constant at 10m):**

**Observation:**

-   Diameter has a **strong accelerating effect** (β₁ = 2.18)

-   Each 5 cm increase in diameter adds increasingly more biomass

-   This reflects the **power-law relationship** (weight ∝ diameter\^2.18)

**2. Effect of Height (holding diameter constant at 22.5 cm):**

**Observation:**

-   Height has a **moderate, decelerating effect** (β₂ = 0.53)

-   Each 5 m increase adds progressively less biomass

-   This reflects the **sub-linear relationship** (weight ∝ height\^0.53)

**3. Extreme Cases:**

**Smallest tree (12.5 cm × 10 m):**

-   Predicted weight: **52.1 kg**

-   Represents a young or suppressed beech tree

**Largest tree (42.5 cm × 40 m):**

-   Predicted weight: **1,559 kg** (from full table)

-   Represents a mature, dominant canopy tree

-   **30× more biomass** than smallest tree

**Visualization Interpretation:**

**Key Patterns in the Graph:**

**Fan-shaped pattern:**

-   Lines diverge as diameter increases

-   Height effect becomes **more pronounced** at larger diameters

-   This is the **interaction effect** of the multiplicative (power-law) model

**Color gradient (height effect):**

-   **Red (10m):** Lowest biomass for any diameter

-   **Pink (40m):** Highest biomass for any diameter

-   Clear **vertical separation** showing height's contribution

**Steep slopes:**

-   All lines are **curved upward** (exponential appearance on original scale)

-   Reflects the strong diameter effect (β₁ = 2.18)

**Biological realism:**

-   Predictions follow **realistic patterns** for tree growth

-   Small trees: 50-150 kg (realistic for young beeches)

-   Large trees: 500-1,500 kg (realistic for mature beeches)

# Case Study 2: Fruit Trees

*Data and ideas for this case study come from Militino et al. (2006).*

To estimate the total surface occupied by fruit trees in three small areas (R63, R67, and R68) of Navarra in 2001, a sample of 47 square segments has been taken. The experimental units are square segments or quadrats of 4 hectares, obtained by random sampling after overlaying a square grid on the study domain. The focus of this case study is illustrating two different techniques used to obtain estimates: direct estimation and small area estimation. The direct technique estimates the total surface area by multiplying the mean of the observed surface area in the sampled segments by the total number of segments in every small area. The small area technique consists of creating a regression model where the dependent variable is the observed surface area occupied by fruit trees in every segment and the explanatory variables are the classified cultivars by satellite in the same segment and the small areas to which they belong. The final surface area totals are obtained by multiplying the total classified surface area of every small area by the β’s parameter estimates obtained from the regression model (observed surface area∼ classified surface area + small areas). The surface variables in the data frame `SATFRUIT` are given in $m^2$:

• `quadrat` is the number of the sampled segment or quadraz

• `smallarea` are the small areas’ labels

• `wheat` is the classified surface of wheat in the sampled segment

• `barley` is the classified surface of barley in the sampled segment

• `nonarable` is the classified surface of fallow or non-arable land in the sampled segment

• `corn` is the classified surface of corn in the sampled segment

• `sunflower` is the classified surface of sunflowers in the sampled segment

• `vineyard` is the classified surface of vineyards in the sampled segment

• `grass` is the classified surface of grass in the sampled segment

• `asparagus` is the classified surface of asparagus in the sampled segment

• `alfalfa` is the classified surface of lucerne (type of alfalfa) in the sampled segment

• `rape` is the classified surface of rape Brassica napus in the sampled segment

• `rice` is the classified surface of rice in the sampled segment

• `almonds` is the classified surface of almonds in the sampled segment

• `olives` is the classified surface of olives in the sampled segment

• `fruit` is the classified surface of fruit trees in the sampled segment

• `observed` is the observed surface of fruit trees in the sampled segment

```{r}
# Load required libraries
suppressPackageStartupMessages({
  library(PASWR2)
  library(car)
  library(boot)
  library(ggplot2)
  library(MASS)
  library(dplyr)
})

# Load the data
data(SATFRUIT)
```

```{r}
head(SATFRUIT)
```

### (a) Characterize the shape, center, and spread for the variable `fruit`.

```{r}
summary(SATFRUIT$fruit)
cat("\nStandard Deviation:", sd(SATFRUIT$fruit), "\n")
cat("IQR:", IQR(SATFRUIT$fruit), "\n")

# Histogram
hist(SATFRUIT$fruit, main = "Distribution of Classified Fruit Surface Area",
     xlab = "Fruit (m²)", col = "lightblue", breaks = 15)
```

**Shape:**

-   **Bimodal distribution**: The histogram shows two distinct peaks or modes:

    -   A smaller peak around 4,000-5,000 m²

    -   A larger, more prominent peak around 12,000-13,000 m²

-   **Irregular/Non-uniform**: The distribution is not smoothly bell-shaped but rather shows multiple peaks and valleys, suggesting heterogeneity in the data (likely due to different small areas having different fruit coverage patterns)

-   **Slight gap/valley** in the middle range (around 6,000-8,000 m²), which separates the two modal regions

-   The distribution suggests there may be **two distinct groups** of segments: those with lower fruit coverage (0-5,000 m²) and those with higher fruit coverage (10,000-14,000 m²)

**Center:**

From the summary statistics provided earlier:

-   **Mean**: 7,827 m² - falls in the valley between the two modes, which is typical for bimodal distributions

-   **Median**: 8,536 m² - slightly higher than the mean, positioned in the lower-density middle region

The center measures are somewhat misleading for a bimodal distribution, as they don't represent either of the typical values well.

**Spread:**

-   **Range**: 0 to 13,969 m² - showing substantial variability (approximately 14,000 m² span)

-   **Standard Deviation**: 4,119.28 m² - indicates considerable dispersion around the mean

-   **IQR**: 7,115.129 m²

    -   Q1: 4,241 m² (within the lower mode region)

    -   Q3: 11,356 m² (within the upper mode region)

The large spread, combined with the bimodal shape, suggests that the sampled segments come from areas with quite different fruit tree coverage characteristics - which aligns with the fact that data comes from three different small areas (R63, R67, R68).(b) What is the maximum number of $m^2$ of classified fruits by segment?

```{r}
cat("Maximum m² of classified fruits:", max(SATFRUIT$fruit), "\n")
```

### (c) How many observations are there by small area?

```{r}
table(SATFRUIT$smallarea)
```

### (d) Use `scatterplotMatrix()` from `car` or `pairs()` to explore the linear relationships between observed and the remainder of the numerical variables. Comment on the results.

```{r}
# Select numerical variables
num_vars <- c("observed", "wheat", "barley", "nonarable", "corn", 
              "sunflower", "vineyard", "grass", "asparagus", 
              "alfalfa", "rape", "rice", "almonds", "olives", "fruit")

# Create simple pairs plot (avoids smoother warnings)
pairs(~ observed + fruit + almonds + olives + vineyard, data = SATFRUIT,
      main = "Pairs Plot - Key Variables",
      pch = 19, col = as.numeric(SATFRUIT$smallarea))
legend("topright", legend = levels(SATFRUIT$smallarea), 
       col = 1:3, pch = 19, cex = 0.8)
```

**Strong Positive Relationships:**

1.  **observed vs. fruit**: Shows the **strongest positive linear relationship**. There's a clear upward trend where higher classified fruit surface area corresponds to higher observed fruit surface area. This makes intuitive sense as these are satellite-classified vs. ground-observed measurements of the same feature. The relationship appears roughly linear across most of the range.

**Moderate to Weak Relationships:**

2.  **observed vs. almonds**: Shows a **weak to moderate positive relationship**. Most almond values are concentrated near zero, but there appears to be some positive association. The relationship is less clear than with fruit.

3.  **observed vs. olives**: Shows a **weak positive relationship**, primarily driven by area R68 (green points). R68 appears to have higher olive coverage, while R67 (pink points) has minimal olive presence. The relationship is not strongly linear overall.

4.  **observed vs. vineyard**: Shows **very weak or no clear linear relationship**. Most vineyard values are clustered near zero with a few outliers. There doesn't appear to be a meaningful linear association with observed fruit.

**Key Patterns by Small Area:**

-   **R67 (pink)**: Tends to have higher observed values and higher fruit values, with minimal olives and vineyards

-   **R68 (green)**: Shows more variability, with some segments having substantial olive coverage

-   The different small areas show distinct patterns, suggesting that area-specific effects may be important

### (e) Create density plots of the observed fruits’ surface area (`observed`) by small areas (`smallarea`).

```{r}
print(ggplot(SATFRUIT, aes(x = observed, fill = smallarea)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density of Observed Fruit Surface by Small Area",
       x = "Observed Surface (m²)", y = "Density") +
  theme_minimal())
```

**Distribution Characteristics by Small Area:**

**R63 (Pink/Salmon):**

-   Shows a **highly concentrated distribution near zero** with a sharp, narrow peak

-   The density is highest at very low observed values (close to 0 m²)

-   Has the **smallest range** and lowest observed fruit surface areas overall

-   Distribution is **strongly right-skewed** with very little spread

-   This suggests R63 has minimal fruit tree coverage in most segments

**R67 (Green):**

-   Displays a **broad, roughly symmetric distribution** centered around **10,000-11,000 m²**

-   Has the **highest median/center** of the three areas

-   Shows **moderate spread** with values ranging approximately from 7,000 to 13,000 m²

-   The distribution is relatively **unimodal and bell-shaped**

-   Indicates R67 has consistently high fruit tree coverage across segments

**R68 (Blue):**

-   Shows a **bimodal or irregular distribution** with peaks around **2,000-3,000 m²** and **5,000-6,000 m²**

-   Has **intermediate values** between R63 and R67

-   Distribution is **moderately spread** with considerable variability

-   Suggests more heterogeneity in fruit tree coverage within R68

**Key Comparisons:**

-   **R67 clearly has the highest observed fruit surface area** with values concentrated in the upper range (8,000-12,000 m²)

-   **R63 has the lowest observed values**, clustered near zero

-   **R68 shows intermediate and more variable coverage**

-   The three areas show **distinctly different distributions**, suggesting that small area membership is an important factor in predicting observed fruit surface area

-   There is **minimal overlap** between R63 and R67, indicating these areas are quite different in terms of fruit tree presence

### (f) Use box plots and bar plots with standard errors to compare the observed surface area (`observed`) and the classified surface area (`fruit`) by small areas (`smallarea`).

```{r}
# Box plots
par(mfrow = c(1, 2))
boxplot(observed ~ smallarea, data = SATFRUIT, 
        main = "Observed Surface by Small Area",
        xlab = "Small Area", ylab = "Observed (m²)", col = "lightblue")

boxplot(fruit ~ smallarea, data = SATFRUIT,
        main = "Classified Surface by Small Area",
        xlab = "Small Area", ylab = "Fruit (m²)", col = "lightgreen")

# Bar plots with standard errors
summary_data <- SATFRUIT %>%
  group_by(smallarea) %>%
  summarise(
    mean_observed = mean(observed),
    se_observed = sd(observed) / sqrt(n()),
    mean_fruit = mean(fruit),
    se_fruit = sd(fruit) / sqrt(n())
  )

par(mfrow = c(1, 2))
bp1 <- barplot(summary_data$mean_observed, names.arg = summary_data$smallarea,
        main = "Mean Observed Surface by Area",
        ylab = "Mean Observed (m²)", col = "lightblue", ylim = c(0, max(summary_data$mean_observed) * 1.2))
arrows(x0 = bp1, 
       y0 = summary_data$mean_observed - summary_data$se_observed,
       y1 = summary_data$mean_observed + summary_data$se_observed,
       angle = 90, code = 3, length = 0.1)

bp2 <- barplot(summary_data$mean_fruit, names.arg = summary_data$smallarea,
        main = "Mean Classified Surface by Area",
        ylab = "Mean Fruit (m²)", col = "lightgreen", ylim = c(0, max(summary_data$mean_fruit) * 1.2))
arrows(x0 = bp2,
       y0 = summary_data$mean_fruit - summary_data$se_fruit,
       y1 = summary_data$mean_fruit + summary_data$se_fruit,
       angle = 90, code = 3, length = 0.1)

```

**Box Plots Analysis:**

**Observed Surface Area (Left, Blue):**

-   **R63**: Shows very low values, with a median near 0 m² and maximum around 5,000-6,000 m². The distribution is highly skewed with most values concentrated near zero.

-   **R67**: Shows the highest observed values, with a median around 11,000 m² and a wide interquartile range (approximately 5,000-12,000 m²). This area has the greatest variability.

-   **R68**: Shows intermediate values with a median around 7,000 m² and an interquartile range from approximately 4,000-10,000 m².

**Classified Surface Area (Right, Green):**

-   **R63**: Very low values, median near 0 m², with maximum around 3,000 m². Similar to observed, most segments have minimal fruit tree coverage.

-   **R67**: Highest classified values with median around 9,000 m² and wide spread (approximately 5,000-12,000 m²).

-   **R68**: Median around 10,000 m² with interquartile range from approximately 8,000-12,000 m². More consistent than observed values.

**Bar Plots with Standard Errors Analysis:**

**Mean Observed Surface:**

-   **R63**: Approximately 2,000 m² (very small, with relatively small standard error)

-   **R67**: Approximately 8,500-9,000 m² (highest mean, with larger standard error indicating high variability)

-   **R68**: Approximately 6,500 m² (intermediate, moderate standard error)

**Mean Classified Surface:**

-   **R63**: Approximately 1,500 m² (smallest, low standard error)

-   **R67**: Approximately 7,500 m² (moderate, with standard error)

-   **R68**: Approximately 9,500-10,000 m² (highest mean classified, moderate standard error)

### (g) Compute the correlation between `observed` and all other numerical variables. List the three variables in order along with their correlation coefficients that have the highest correlation with `observed`.

```{r}
correlations <- cor(SATFRUIT[, num_vars[-1]], SATFRUIT$observed)
correlations_sorted <- sort(abs(correlations), decreasing = TRUE)

cat("\nTop 3 variables with highest correlation with 'observed':\n")
print(head(correlations_sorted, 3))
cat("\nWith signs:\n")
print(correlations[names(head(correlations_sorted, 3))])
```

**Top 3 Variables with Highest Correlation with 'observed':**

Based on the correlation magnitudes shown:

1.  **fruit**: r = 0.819 (very strong positive correlation)

    -   This is by far the strongest correlation, which makes sense as both variables measure fruit tree surface area - one from satellite classification (fruit) and one from ground observation (observed)

2.  **Second variable**: r = 0.402 (moderate positive correlation)

    -   Based on the scatterplot matrix, this is likely **almonds**

3.  **Third variable**: r = 0.399 (moderate positive correlation)

    -   Based on the scatterplot matrix, this is likely **olives**

**Interpretation:**

-   The **fruit** variable has a strong correlation of 0.819, explaining approximately 67% of the variance in observed values (r² = 0.67)

-   The other two variables show much weaker correlations (around 0.40), indicating they are far less predictive individually

-   The large drop from 0.819 to 0.40 suggests that **fruit** is the dominant predictor

-   These correlations align with what we observed in the scatterplot matrix, where fruit showed a clear linear relationship while almonds and olives showed weaker, more scattered relationships

## Model (A)

Use backward elimination to develop a model that predicts `observed` using the data frame `SATFRUIT` without considering `smallarea`. Start the backward elimination process by considering all of the numerical variables in `SATFRUIT` as potential predictors. Use a p-value-to-remove of 10%. Store the final model in the object `modelA`.

```{r}
# Start with full model (excluding quadrat and smallarea)
predictors <- setdiff(num_vars, c("observed"))
formula_full <- as.formula(paste("observed ~", paste(predictors, collapse = " + ")))

model_full <- lm(formula_full, data = SATFRUIT)

# Backward elimination with p = 0.10
# k = qchisq(0.10, 1, lower.tail = FALSE) for p-value criterion
modelA <- step(model_full, direction = "backward", 
               k = qchisq(0.10, 1, lower.tail = FALSE),
               trace = 0)

cat("\nModel A Summary:\n")
summary(modelA)

# Cross-validation for Model A
modelA_glm <- glm(formula(modelA), data = SATFRUIT)
```

#### i. Compute $CV_n$, the leave-one-out cross-validation error, for `modelA`. Set the seed to 5 and compute $CV_5$, the five-fold cross-validation error, for `modelA`. The cross-validation error for a generalized linear model can be computed using the `cv.glm()` function from the boot package. Using the function `glm()` without passing a family argument is equivalent to using the function `lm()`. R Code 1 provides a template for how to use the `cv.glm()` function. Note that $CV_n$ is returned with `cv.error$delta[1]`. To compute $CV_5$, pass the value 5 to the argument $K$ inside the `cv.glm()` function.

```{=plain}
R Code 1:
> mod.glm <- glm(y ~ x1 + x2, data = DF)
> library(boot)
> cv.error <- cv.glm(data = DF, glmfit = mod.glm)
> cv.error$delta[1]
```
```{r}
# LOOCV (CV_n)
cv_n_A <- cv.glm(data = SATFRUIT, glmfit = modelA_glm)
CVn_A <- cv_n_A$delta[1]

# 5-fold CV
set.seed(5)
cv_5_A <- cv.glm(data = SATFRUIT, glmfit = modelA_glm, K = 5)
CV5_A <- cv_5_A$delta[1]

cat("\n--- Model A Cross-Validation ---\n")
cat("CV_n (LOOCV):", CVn_A, "\n")
cat("CV_5 (5-fold):", CV5_A, "\n")
```

#### ii. Compute $R^2, R^2_a$, the AIC, and the BIC for Model (A). What is the proportion of total variability explained by Model (A)?

```{r}
# Model A statistics
summary_A <- summary(modelA)
R2_A <- summary_A$r.squared
R2a_A <- summary_A$adj.r.squared
AIC_A <- AIC(modelA)
BIC_A <- BIC(modelA)

cat("\n--- Model A Statistics ---\n")
cat("R²:", R2_A, "\n")
cat("Adjusted R²:", R2a_A, "\n")
cat("AIC:", AIC_A, "\n")
cat("BIC:", BIC_A, "\n")
cat("Proportion of variability explained:", R2_A, "\n")
```

## Model (B)

Use the criterion-based procedure AIC, which for linear regression is equivalent to Mallow’s Cp, to develop a model that predicts `observed` using all of the numerical variables in `SATFRUIT`. Store the model in the object `modelB`. Verify that the model suggested using BIC is the same model as the one suggested by AIC or Mallow’s Cp, which are all the same as Model (A).

```{r}
modelB <- stepAIC(model_full, direction = "both", trace = 0)
cat("\nModel B Summary:\n")
summary(modelB)

# Verify BIC gives same model
modelB_BIC <- stepAIC(model_full, direction = "both", k = log(nrow(SATFRUIT)), trace = 0)

cat("\nModel B (AIC) formula:", deparse(formula(modelB)), "\n")
cat("Model with BIC formula:", deparse(formula(modelB_BIC)), "\n")
cat("Are Model B and Model A the same?", 
    identical(sort(names(coef(modelA))), sort(names(coef(modelB)))), "\n")
```

**Verification: BIC Gives Same Model as Model A**

The verification **failed**:

-   **BIC model** = grass + olives + fruit (same as Model A) ✓

-   **AIC model** = grass + almonds + olives + fruit (Model B) ✗

-   **AIC ≠ Model A**

**Conclusion:**

The problem statement's expectation was incorrect. In this case:

-   **AIC and BIC selected different models**

-   BIC correctly selected the more parsimonious Model A

-   AIC selected Model B with the non-significant `almonds` variable

-   This demonstrates the difference between AIC (prediction-focused) and BIC (parsimony-focused)

Model B's inclusion of `almonds` (p = 0.170) provides negligible improvement and violates the principle of parsimony. **Use Model A for subsequent analyses unless cross-validation errors strongly favor Model B.**

## Model (C)

Use mean squared prediction error (MSPE) to select a model using all of the numerical variables in `SATFRUIT` as potential predictors for predicting observed. Store the model in the object `modelC`. Specifically, select a model using both leave-one-out cross validation (LOOCV) and five-fold cross validation.

```{r}
# Test various model combinations based on correlations
# and select based on CV error

candidate_models <- list(
  m1 = observed ~ fruit,
  m2 = observed ~ fruit + almonds,
  m3 = observed ~ fruit + almonds + olives,
  m4 = observed ~ fruit + olives,
  m5 = observed ~ fruit + almonds + olives + vineyard,
  m6 = observed ~ fruit + vineyard
)
```

#### 1. Compute $CV_n$ for `modelC`. Set the seed to 5 and compute $CV_5$ for `modelC`.

```{r}
# Function to compute CV errors
compute_cv <- function(formula, data, K = NULL) {
  mod_glm <- glm(formula, data = data)
  if(is.null(K)) {
    cv_result <- cv.glm(data = data, glmfit = mod_glm)
  } else {
    set.seed(5)
    cv_result <- cv.glm(data = data, glmfit = mod_glm, K = K)
  }
  return(cv_result$delta[1])
}

# Compare CV errors for candidate models
cv_results <- data.frame(
  Model = names(candidate_models),
  Formula = sapply(candidate_models, deparse),
  CV_n = sapply(candidate_models, compute_cv, data = SATFRUIT),
  CV_5 = sapply(candidate_models, function(f) compute_cv(f, SATFRUIT, K = 5))
)

print(cv_results[, c("Model", "CV_n", "CV_5")])
```

#### 2. Compute R2, R2a, the AIC, and the BIC for Model (C). What is the proportion of total variability explained by Model (C)?

```{r}
# Select model with minimum CV error (use CV_n for selection)
best_idx <- which.min(cv_results$CV_n)
cat("\nBest model based on LOOCV:", cv_results$Model[best_idx], "\n")
cat("Formula:", cv_results$Formula[best_idx], "\n")

modelC <- lm(candidate_models[[best_idx]], data = SATFRUIT)
cat("\nModel C Summary:\n")
summary(modelC)

# Store CV errors for Model C
cv_n_C <- cv_results$CV_n[best_idx]
CV5_C <- cv_results$CV_5[best_idx]

cat("\n--- Model C Cross-Validation ---\n")
cat("CV_n (LOOCV):", cv_n_C, "\n")
cat("CV_5 (5-fold):", CV5_C, "\n")

# Model C statistics
summary_C <- summary(modelC)
R2_C <- summary_C$r.squared
R2a_C <- summary_C$adj.r.squared
AIC_C <- AIC(modelC)
BIC_C <- BIC(modelC)

cat("\n--- Model C Statistics ---\n")
cat("R²:", R2_C, "\n")
cat("Adjusted R²:", R2a_C, "\n")
cat("AIC:", AIC_C, "\n")
cat("BIC:", BIC_C, "\n")
```

**Proportion of Total Variability Explained: 67.03%**

**Interpretation:**

-   **Approximately two-thirds** (67%) of the variation in observed fruit tree coverage can be explained by satellite-classified fruit area alone

-   This is a **strong single-predictor model** - the satellite classification is a good predictor by itself

-   The remaining **33%** of variability is due to:

    -   Measurement errors

    -   Misclassification by satellite

    -   Other land use factors (grass, olives, almonds, etc.)

    -   Small area effects (R63, R67, R68 differences)

## Model (D)

Use whichever of Model (A) or (C) has the smaller cross-validation error, and introduce `smallarea` into the chosen model. Store the new model that includes `smallarea` in `modelD`.

#### (i.) Eliminate any variables from `modelD` that are not statistically significant (α= 0.10). Store the resulting model in `modelD`.

```{r}
# Choose model with smaller CV error
if(CVn_A < cv_n_C) {
  base_model <- modelA
  cat("Using Model A as base (smaller CV error)\n")
} else {
  base_model <- modelC
  cat("Using Model C as base (smaller CV error)\n")
}

# Add smallarea
formula_D <- update(formula(base_model), ~ . + smallarea)
modelD_initial <- lm(formula_D, data = SATFRUIT)

cat("\nInitial Model D with smallarea:\n")
summary(modelD_initial)

# Remove non-significant variables (α = 0.10)
modelD <- step(modelD_initial, direction = "backward", 
               k = qchisq(0.10, 1, lower.tail = FALSE),
               trace = 0)

cat("\nFinal Model D Summary:\n")
summary(modelD)
```

#### (ii.) Compute CVn for modelD. Set the seed to 5 and compute CV5 for modelD.

```{r}
# Cross-validation for Model D
modelD_glm <- glm(formula(modelD), data = SATFRUIT)
cv_n_D <- cv.glm(data = SATFRUIT, glmfit = modelD_glm)$delta[1]
set.seed(5)
cv_5_D <- cv.glm(data = SATFRUIT, glmfit = modelD_glm, K = 5)$delta[1]

cat("\n--- Model D Cross-Validation ---\n")
cat("CV_n (LOOCV):", cv_n_D, "\n")
cat("CV_5 (5-fold):", cv_5_D, "\n")
```

#### (iii.) Compute R2, R2a, the AIC, and the BIC for Model (D). What is the proportion of total variability explained by Model (D)?

```{r}
# Model D statistics
summary_D <- summary(modelD)
R2_D <- summary_D$r.squared
R2a_D <- summary_D$adj.r.squared
AIC_D <- AIC(modelD)
BIC_D <- BIC(modelD)

cat("\n--- Model D Statistics ---\n")
cat("R²:", R2_D, "\n")
cat("Adjusted R²:", R2a_D, "\n")
cat("AIC:", AIC_D, "\n")
cat("BIC:", BIC_D, "\n")
```

#### (iv.) Does Model (D) have a smaller cross-validation error than the cross-validation error for either Model (A) or Model (C)?

```{r}
# Compare CV errors
cat("\n--- CV Error Comparison ---\n")
cat("Model A CV_n:", CVn_A, "\n")
cat("Model C CV_n:", cv_n_C, "\n")
cat("Model D CV_n:", cv_n_D, "\n")
cat("\nDoes Model D have smaller CV error than both A and C?", 
    cv_n_D < min(CVn_A, cv_n_C), "\n")
```

#### (v.) Plot the Cook distances, the studentized residuals, the diagonal elements of the hat matrix, the DFFITS, and DFBETAS1 of Model (D) versus the index.

```{r}
par(mfrow = c(2, 3))

# Cook's distance
cooks_d <- cooks.distance(modelD)
plot(cooks_d, type = "h", 
     main = "Cook's Distance", ylab = "Cook's D", xlab = "Index")
abline(h = 4/nrow(SATFRUIT), col = "red", lty = 2)
text(x = which(cooks_d > 4/nrow(SATFRUIT)), 
     y = cooks_d[cooks_d > 4/nrow(SATFRUIT)],
     labels = which(cooks_d > 4/nrow(SATFRUIT)), pos = 3, cex = 0.7)

# Studentized residuals
rstud <- rstudent(modelD)
plot(rstud, type = "h",
     main = "Studentized Residuals", ylab = "Studentized Residuals", xlab = "Index")
abline(h = c(-2, 2), col = "red", lty = 2)
text(x = which(abs(rstud) > 2), 
     y = rstud[abs(rstud) > 2],
     labels = which(abs(rstud) > 2), pos = 3, cex = 0.7)

# Hat values
hat_vals <- hatvalues(modelD)
leverage_threshold <- 2 * length(coef(modelD)) / nrow(SATFRUIT)
plot(hat_vals, type = "h",
     main = "Hat Values (Leverage)", ylab = "Hat Values", xlab = "Index")
abline(h = leverage_threshold, col = "red", lty = 2)
text(x = which(hat_vals > leverage_threshold), 
     y = hat_vals[hat_vals > leverage_threshold],
     labels = which(hat_vals > leverage_threshold), pos = 3, cex = 0.7)

# DFFITS
dffits_vals <- dffits(modelD)
dffits_threshold <- 2 * sqrt(length(coef(modelD)) / nrow(SATFRUIT))
plot(dffits_vals, type = "h",
     main = "DFFITS", ylab = "DFFITS", xlab = "Index")
abline(h = c(-dffits_threshold, dffits_threshold), col = "red", lty = 2)

# DFBETAS for first predictor (fruit coefficient)
dfbetas_vals <- dfbetas(modelD)[, 2]
dfbetas_threshold <- 2 / sqrt(nrow(SATFRUIT))
plot(dfbetas_vals, type = "h",
     main = "DFBETAS (fruit)", ylab = "DFBETAS", xlab = "Index")
abline(h = c(-dfbetas_threshold, dfbetas_threshold), col = "red", lty = 2)
```

1.  Cook's Distance

What it measures: Overall influence of each observation on the model

Interpretation: Most points are below the threshold, indicating no single observation has excessive influence on the entire model. A few observations approach the line but don't exceed it significantly.

2.  Studentized Residuals

What it measures: Standardized prediction errors

Interpretation: A few observations exceed ±2 (particularly one around -3), suggesting some potential outliers in the predictions. However, these are relatively few and within acceptable limits for a dataset of this size.

3.  Hat Values (Leverage)

What it measures: How unusual an observation's predictor values are

Interpretation: Three observations (10, 22, 45) exceed this threshold, meaning they have unusual combinations of predictor values (fruit area and/or small area category). These are leverage points.

4.  DFFITS

What it measures: How much fitted values change when an observation is removed

Interpretation: Most observations fall within bounds, meaning individual observations don't drastically change their own predicted values when removed from the model.

5.  DFBETAS for Intercept

What it measures: How much the intercept coefficient changes when an observation is removed

Interpretation: A few observations slightly exceed these bounds, suggesting they have some influence on the intercept, but effects are modest.

6.  Residuals vs Fitted

What it checks: Linearity and homoscedasticity (constant variance) Interpretation: Points scatter randomly around zero with relatively constant spread across fitted values, supporting model assumptions. No clear patterns or funnel shapes that would indicate problems.

#### (vi.) Are there any leverage points? Justify the answer given.

```{r}
leverage_points <- which(hat_vals > leverage_threshold)
cat("Leverage threshold (2p/n):", leverage_threshold, "\n")
cat("Number of leverage points:", length(leverage_points), "\n")
cat("Leverage points (observations):", leverage_points, "\n")
if(length(leverage_points) > 0) {
  cat("\nJustification: These observations have hat values exceeding 2p/n =", 
      round(leverage_threshold, 4), "\n")
}
```

**Justification**

**Definition of Leverage:**

Leverage (hat values) measures how **unusual or extreme** an observation's predictor values are compared to the rest of the data. High leverage points are observations that are **far from the center** of the predictor space.

**Why These Are Leverage Points:**

These three observations (10, 22, 45) have **hat values that exceed the threshold of 0.1702**, meaning they have:

1.  **Unusual combinations of predictor values** (likely extreme values of `fruit` and/or unusual `smallarea` combinations)

2.  **Greater potential to influence** the fitted regression line

3.  **Disproportionate weight** in determining the regression coefficients

**Interpreting the Hat Values from the Plot**

Looking at the **Hat Values (Leverage)** plot in the image:

| **Observation** | **Approximate Hat Value** | **Exceeds Threshold?** |
|-----------------|---------------------------|------------------------|
| **#10**         | \~**0.35**                | ✓ YES (2× threshold)   |
| **#22**         | \~**0.35**                | ✓ YES (2× threshold)   |
| **#45**         | \~**0.35**                | ✓ YES (2× threshold)   |

All three leverage points have hat values approximately **0.35**, which is:

-   **More than double** the threshold (0.35 / 0.1702 ≈ 2.06×)

-   Among the **highest leverage values** in the dataset

#### (vii.) Are there any outliers? Justify the answer given.

```{r}
outliers <- which(abs(rstud) > 2)
cat("Number of outliers (|rstudent| > 2):", length(outliers), "\n")
cat("Outliers (observations):", outliers, "\n")
if(length(outliers) > 0) {
  cat("\nJustification: These observations have studentized residuals with absolute value > 2\n")
  cat("Studentized residuals for outliers:\n")
  print(rstud[outliers])
}
```

**Justification**

**Definition of Outliers:**

Outliers are observations with **unusually large residuals** - their observed values are far from what the model predicts. Studentized residuals standardize the residuals to account for their varying standard errors, making them comparable across observations.

**Why These Are Outliers:**

Both observations #3 and #46 have **studentized residuals with absolute values exceeding 2**, specifically:

1.  **Observation #3**:

    -   Studentized residual = **-2.132**

    -   Exceeds the threshold of 2 by about **6.6%**

    -   **Moderately unusual** observation

2.  **Observation #46**:

    -   Studentized residual = **-3.031**

    -   Exceeds the threshold of 2 by about **51.5%**

    -   **Highly unusual** observation - exceeds even the stricter threshold of \|t\| \> 3

**Interpretation of Negative Residuals**

**What Does a Negative Studentized Residual Mean?**

**Residual = Observed - Predicted**

Since both outliers have **negative** studentized residuals:

-   The **observed fruit area is LESS than predicted**

-   The model **overpredicts** fruit tree coverage for these segments

-   These observations have **unexpectedly low** observed fruit area given their predictor values

**Detailed Analysis by Observation**

**Observation #3 (Moderate Outlier):**

**Studentized Residual**: -2.132

**Characteristics**:

-   Moderately extreme (2-3 standard deviations below expected)

-   Model predicts higher fruit coverage than actually observed

-   Could indicate:

    -   Satellite **misclassification** (classified as fruit but actually something else)

    -   Measurement error in ground observations

    -   Genuine unusual case (e.g., recently cleared fruit trees)

**Severity**: Moderate concern

**Observation #46 (Severe Outlier):**

**Studentized Residual**: -3.031

**Characteristics**:

-   **Highly extreme** (\>3 standard deviations below expected)

-   This is the **most problematic outlier** in the dataset

-   Only **\~0.3%** of observations should exceed \|t\| \> 3 in a normal distribution

-   Model **significantly overpredicts** for this observation

**Possible Explanations**:

1.  **Data entry error**: Typo in observed or predictor values

2.  **Measurement error**: Ground observation was inaccurate

3.  **Satellite misclassification**: Large area classified as fruit but is not

4.  **Temporal mismatch**: Satellite data and ground observation from different times

5.  **Legitimate unusual case**: Unique characteristics (e.g., diseased orchard, recent harvest)

**Severity**: High concern - should be investigated

**Visual Confirmation from Plot**

Looking at the **Studentized Residuals** plot (top middle of diagnostic plots):

**Observations:**

-   Most residuals cluster between **-1 and +1** ✓

-   Red dashed lines mark the **±2 threshold**

-   **Two observations clearly breach the lower threshold**:

    -   One around index 3: residual ≈ **-2.1** (matches observation #3)

    -   One around index 46: residual ≈ **-3.0** (matches observation #46, labeled in plot)

-   **No observations exceed +2** on the positive side

-   The outliers are on the **negative side only**, indicating systematic underprediction for these cases

#### (viii) Check normality and homoscedasticity for Model (D) using graphics and hypothesis tests.

```{r}
par(mfrow = c(2, 2))
plot(modelD)

# Shapiro-Wilk test for normality
shapiro_test <- shapiro.test(residuals(modelD))
cat("\nShapiro-Wilk test for normality of residuals:\n")
cat("W =", shapiro_test$statistic, ", p-value =", shapiro_test$p.value, "\n")
if(shapiro_test$p.value > 0.05) {
  cat("Conclusion: Fail to reject null hypothesis. Residuals appear normally distributed (α=0.05).\n")
} else {
  cat("Conclusion: Reject null hypothesis. Evidence of non-normality in residuals (α=0.05).\n")
}

# Breusch-Pagan test for homoscedasticity
bp_test <- ncvTest(modelD)
cat("\nBreusch-Pagan test for homoscedasticity:\n")
cat("Chi-square =", bp_test$ChiSquare, ", df =", bp_test$Df, ", p-value =", bp_test$p, "\n")
if(bp_test$p > 0.05) {
  cat("Conclusion: Fail to reject null hypothesis. No evidence of heteroscedasticity (α=0.05).\n")
} else {
  cat("Conclusion: Reject null hypothesis. Evidence of heteroscedasticity (α=0.05).\n")
}
```

**NORMALITY:**

**Hypothesis Test:**

**Shapiro-Wilk Test**:

-   **W = 0.9568, p-value = 0.080**

-   **Conclusion**: At α = 0.05, we **fail to reject the null hypothesis**

-   **Interpretation**: The residuals **appear normally distributed**

**Visual Assessment:**

**Q-Q Plot**:

-   Points follow the diagonal line closely throughout most of the range

-   Minor deviation in left tail due to outliers (#3, #46)

-   **Conclusion**: **Normality assumption is reasonably satisfied**

**HOMOSCEDASTICITY:**

**Hypothesis Test:**

**Breusch-Pagan Test**:

-   **χ² = 0.3484, df = 1, p-value = 0.555**

-   **Conclusion**: At α = 0.05, we **fail to reject the null hypothesis**

-   **Interpretation**: **No evidence of heteroscedasticity** - variance is constant

**Visual Assessment:**

**Residuals vs Fitted Plot**:

-   Random scatter around horizontal line at zero

-   Constant vertical spread across fitted values

-   **Conclusion**: **Homoscedasticity assumption is satisfied**

**Scale-Location Plot**:

-   Horizontal smoothed line with no strong trend

-   Even vertical spread of √\|standardized residuals\|

-   **Conclusion**: **Confirms constant variance**

**OVERALL CONCLUSION: Both normality and homoscedasticity assumptions are satisfied for Model D.**

#### (ix.) Calculate a 95% confidence interval for the fruit coefficient.

```{r}
ci_fruit <- confint(modelD, "fruit", level = 0.95)
cat("95% Confidence Interval for fruit coefficient:\n")
print(ci_fruit)
cat("\nInterpretation: We are 95% confident that the true coefficient for fruit is between",
    round(ci_fruit[1], 4), "and", round(ci_fruit[2], 4), "\n")
```

### (h) How many hectares of observed fruits are expected to be incremented if the classified hectares of fruit trees by the satellite are increased by 10,000 m2 (1 ha)?

```{r}
fruit_coef <- coef(modelD)["fruit"]
increase_m2 <- fruit_coef * 10000
increase_ha <- increase_m2 / 10000

cat("Fruit coefficient:", fruit_coef, "\n")
cat("Expected increase in observed for 10,000 m² increase in classified fruit:", 
    increase_m2, "m²\n")
cat("In hectares:", increase_ha, "ha\n")
```

### (i) Suppose the total classified fruits by the satellite in area R63 is 97,044.28 m2, in area R67 is 4,878,603.43 m2, and in area R68 is 2,883,488.24 m2. Predict the total area of fruit trees by small areas.

```{r}
# Total classified fruits by area
total_classified <- data.frame(
  smallarea = factor(c("R63", "R67", "R68"), levels = levels(SATFRUIT$smallarea)),
  fruit = c(97044.28, 4878603.43, 2883488.24)
)

predictions <- predict(modelD, newdata = total_classified)
cat("\nPredicted total observed area by small area:\n")
for(i in 1:3) {
  cat(total_classified$smallarea[i], ":", round(predictions[i], 2), "m² (",
      round(predictions[i]/10000, 2), "ha)\n")
}
```

### (j) Create a plot of observed versus fruit with the points color coded according to small area. Superimpose the corresponding regression lines for each small area.

```{r}
p_scatter <- ggplot(SATFRUIT, aes(x = fruit, y = observed, color = smallarea)) +
  geom_point(size = 2) +
  geom_smooth(method = "lm", se = TRUE, alpha = 0.2) +
  labs(title = "Observed vs Classified Fruit Surface by Small Area",
       x = "Classified Fruit (m²)", 
       y = "Observed Fruit (m²)",
       color = "Small Area") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(p_scatter)
```

**Summary of the Plot:**

The plot successfully displays:

**Scatterplot**: Observed (y-axis) vs Classified (x-axis) fruit surface area

**Color Coding**:

-   R63 (red): 2 points, near origin

-   R67 (green): \~27 points, highest coverage

-   R68 (blue): \~18 points, intermediate coverage

**Area-Specific Regression Lines**:

-   Each small area has its **own regression line** with different slope and intercept

-   **R67 (green)**: Steepest slope, tightest fit

-   **R68 (blue)**: Moderate slope, more scatter

-   **R63 (red)**: Limited data, uncertain relationship

**Confidence Bands**: Shaded regions show 95% confidence intervals for each regression line

**Key Findings:**

1.  **Strong positive correlation** between satellite-classified and ground-observed fruit coverage in all areas

2.  **Area-specific relationships** exist:

    -   Different intercepts (baseline fruit coverage)

    -   Different slopes (classification accuracy)

    -   Different precision (variance around line)

3.  **R67 shows best agreement** between satellite and ground data (tight clustering)

4.  **R68 shows more variability** with several outliers (points far below line)

5.  **R63 has minimal fruit coverage** with insufficient data for reliable estimation

6.  **Visual justification for Model D**: The plot demonstrates why including `smallarea` improves predictions—the relationship between classified and observed fruit varies systematically across regions

### (k) Plot the individual predictions for model D versus the observed data. Add a diagonal line to the plot.

```{r}
pred_D <- predict(modelD)
par(mfrow = c(1, 1))
plot(pred_D, SATFRUIT$observed, 
     main = "Predicted vs Observed (Model D)",
     xlab = "Predicted", ylab = "Observed",
     pch = 19, col = as.numeric(SATFRUIT$smallarea))
abline(a = 0, b = 1, col = "red", lwd = 2, lty = 2)
legend("topleft", legend = levels(SATFRUIT$smallarea), 
       col = 1:3, pch = 19, title = "Small Area")

# Add R² to plot
text(x = min(pred_D), y = max(SATFRUIT$observed),
     labels = paste("R² =", round(R2_D, 3)), pos = 4)
```

The plot displays **Model D's predicted values vs. observed fruit surface area** with the following elements:

**Diagonal Reference Line** (red dashed):

-   Represents perfect prediction (y = x)

-   Points on line = perfect accuracy

-   Points above line = underprediction

-   Points below line = overprediction

**Color-Coded Points by Small Area**:

-   **R63 (black)**: 2 points near origin and low-mid range

-   **R67 (red)**: \~27 points, distributed 0-13,000 m²

-   **R68 (green)**: \~18 points, distributed 0-11,000 m²

**R² = 0.782**: Displayed in plot, indicates 78.2% of variance explained

**Key Findings:**

1.  **Strong Overall Fit**:

    -   Points cluster tightly around the diagonal line

    -   Model D demonstrates **good predictive accuracy**

2.  **Best Performance in R67** (red points):

    -   Extremely tight clustering around diagonal

    -   Particularly excellent for predictions \> 10,000 m²

    -   Very few deviations

3.  **Variable Performance in R68** (green points):

    -   More scatter around diagonal

    -   **Two significant outliers** below the line (overpredictions)

    -   Less reliable than R67

4.  **Limited Data for R63** (black points):

    -   Only 2 points visible

    -   Appear to be reasonably accurate

    -   Insufficient for strong conclusions

5.  **No Systematic Bias**:

    -   Points distributed symmetrically around diagonal

    -   Model is **well-calibrated**

    -   Unbiased predictions overall

6.  **Accuracy by Range**:

    -   **High predictions (\>10,000 m²)**: Excellent accuracy (±500-1,000 m²)

    -   **Mid predictions (5,000-10,000 m²)**: Good accuracy (±1,000-1,500 m²)

    -   **Low predictions (\<5,000 m²)**: More variable (±1,000-2,500 m²)

**Conclusion:**

The predicted vs. observed plot provides **strong visual evidence** that Model D performs well for predicting fruit tree surface area. The tight clustering around the diagonal line, particularly for R67, confirms the model's **78.2% variance explained** and its superior **cross-validation performance** compared to Models A and C.

While some outliers exist (primarily in R68), the overall pattern demonstrates that Model D is **reliable and well-calibrated** for estimating fruit tree coverage across the small areas of Navarra, with the best performance in R67 where the most data is available.

### (l) Create a bar plot that displays the predicted area occupied by fruit trees based on model D for each small area and the direct estimates of the area occupied by fruit trees by small area knowing that the total number of classified segments in areas R63, R67, and R68 are 119, 703, and 564, respectively.

```{r}
# Total segments by area
total_segments <- data.frame(
  smallarea = c("R63", "R67", "R68"),
  n_segments = c(119, 703, 564)
)

# Direct estimates
direct_estimates <- SATFRUIT %>%
  group_by(smallarea) %>%
  summarise(mean_observed = mean(observed)) %>%
  left_join(total_segments, by = "smallarea") %>%
  mutate(total_direct = mean_observed * n_segments)

# Model D predictions (total for each area)
model_predictions <- data.frame(
  smallarea = c("R63", "R67", "R68"),
  total_predicted = predictions
)

comparison <- left_join(direct_estimates, model_predictions, by = "smallarea")

print(comparison)
```

```{r}
# Bar plot
barplot_data <- rbind(comparison$total_direct, comparison$total_predicted)
colnames(barplot_data) <- comparison$smallarea

par(mfrow = c(1, 1))
bp <- barplot(barplot_data, beside = TRUE, 
        names.arg = comparison$smallarea,
        col = c("steelblue", "coral"),
        main = "Direct vs Model D Predictions by Small Area",
        ylab = "Total Area (m²)",
        xlab = "Small Area",
        legend.text = c("Direct Estimate", "Model D Prediction"),
        args.legend = list(x = "topright", bty = "n"),
        ylim = c(0, max(barplot_data) * 1.15))

# Add values on bars
text(x = bp, y = barplot_data, 
     labels = format(round(barplot_data, 0), big.mark = ","),
     pos = 3, cex = 0.8)
```

**Summary of the Bar Plot:**

The plot compares **two estimation methods** for predicting total fruit tree surface area across three small areas (R63, R67, R68):

**Results Table:**

| **Small Area** | **Total Segments** | **Direct Estimate (m²)** | **Model D Prediction (m²)** | **Difference (m²)** | **% Difference** |
|----|----|----|----|----|----|
| **R63** | 119 | 198,466 | 89,989 | +108,477 | +121% |
| **R67** | 703 | 5,867,470 | 4,503,209 | +1,364,261 | +30% |
| **R68** | 564 | 3,589,159 | 2,658,694 | +930,465 | +35% |
| **TOTAL** | 1,386 | **9,655,095** | **7,251,892** | **+2,403,203** | **+33%** |

**Key Findings:**

1.  **Direct Estimation Systematically Overestimates**:

    -   Direct method produces **higher estimates** in all three areas

    -   Overestimation ranges from **30-121%**

    -   Total overestimation: **240 hectares** (2.4 million m²)

2.  **Model D Provides More Conservative Estimates**:

    -   Uses regression with satellite data for **variance reduction**

    -   Accounts for area-specific effects

    -   Superior **cross-validation performance** (lowest CV error)

3.  **Largest Discrepancy in R63**:

    -   Direct estimate is **2.2× higher** than Model D

    -   Due to **very small sample size** (n≈2)

    -   Demonstrates **unreliability of direct method** for small samples

4.  **Substantial Differences Even in R67**:

    -   Despite having most data (n≈27), estimates differ by **136 ha**

    -   Shows value of **small area estimation** even with moderate samples

5.  **Practical Impact**:

    -   Using direct estimates would **overallocate resources** by 33%

    -   Model D provides **more accurate, efficient estimates**

    -   Critical for policy, planning, and resource management

**Conclusion:**

The bar plot provides **compelling visual evidence** for the superiority of the **small area estimation approach (Model D)** over traditional **direct estimation**. By incorporating satellite classification data and area-specific effects, Model D produces more accurate and stable estimates, particularly critical for areas with small sample sizes like R63. The consistent pattern of overestimation by the direct method (totaling 240 hectares) demonstrates the practical value of the regression-based small area estimation methodology described in this case study.

# Case Study 3: Real Estate

*Data and ideas for this case study come from (Militino et al., 2004).*

The goal of this case study is to walk the user through the creation of a parsimonious multiple linear regression model that can be used to predict the total price (`totalprice`) of apartments by their hedonic (structural) characteristics. The data frame `VIT2005` contains several variables, and further description of the data can be found in the help file.

```{r}
# Load required packages
library(car)
library(boot)
library(MASS)
library(leaps)
library(spuRs)

# Load data
data(VIT2005)
```

### (a) Characterize the shape, center, and spread of the variable `totalprice`.

```{r}
summary(VIT2005$totalprice)
cat("\nStandard Deviation:", sd(VIT2005$totalprice), "\n")
cat("Variance:", var(VIT2005$totalprice), "\n")
cat("IQR:", IQR(VIT2005$totalprice), "\n")

# Visualize distribution
par(mfrow = c(1, 2))
hist(VIT2005$totalprice, main = "Histogram of Total Price", 
     xlab = "Total Price (euros)", col = "lightblue", breaks = 15)
boxplot(VIT2005$totalprice, main = "Boxplot of Total Price", 
        ylab = "Total Price (euros)", col = "lightgreen")
par(mfrow = c(1, 1))
```

Shape, Center, and Spread of Total Price

**Center (Central Tendency)**

-   **Mean**: €280,742

-   **Median**: €269,750

The mean is slightly higher than the median (€10,992 difference), suggesting a slight right skew in the distribution.

**Spread (Variability)**

-   **Standard Deviation**: €69,298.46

-   **Variance**: €4,802,276,444

-   **Range**: €405,000 (from €155,000 to €560,000)

-   **Interquartile Range (IQR)**: €100,125

    -   **Q1 (1st Quartile)**: €228,500

    -   **Q3 (3rd Quartile)**: €328,625

The relatively large standard deviation (about 25% of the mean) indicates substantial variability in apartment prices.

**Shape (Distribution)**

**From the Histogram:**

-   The distribution is approximately **bell-shaped** with a slight **right skew** (positive skew)

-   Most apartment prices are concentrated between €200,000 and €350,000

-   The peak (mode) appears to be around €250,000-€270,000

-   There are a few higher-priced apartments extending to €560,000, creating the right tail

**From the Boxplot:**

-   One clear **outlier** is visible above the upper whisker (around €560,000)

-   The box is relatively symmetric, with the median line slightly below center

-   The upper whisker extends further than the lower whisker, confirming right skewness

### (b) Use `scatterplotMatrix()` from the `car` package or `pairs()` to explore the relationships between `totalprice` and the numerical explanatory variables: `area`, `age`, `floor`, `rooms`, `toilets`, `garage`, `elevator`, and `storage`.

```{r}
scatterplotMatrix(VIT2005[, c("totalprice", "area", "age", "floor", "rooms", 
                               "toilets", "garage", "elevator", "storage")], 
                  smooth = FALSE,
                  main = "Scatterplot Matrix")
```

Relationships Between Total Price and Explanatory Variables

**Strong Positive Relationships:**

1.  **Area** (strongest relationship)

    -   Clear, strong positive linear relationship with totalprice

    -   As area increases, total price increases substantially

    -   The scatterplot shows a tight, upward-sloping pattern

    -   This appears to be the strongest predictor

2.  **Toilets**

    -   Moderate to strong positive relationship

    -   More toilets are associated with higher prices

    -   Shows a clear upward trend

3.  **Rooms**

    -   Positive relationship with totalprice

    -   More rooms generally correspond to higher prices

    -   Some clustering at discrete values (since rooms is typically a count variable)

**Moderate Positive Relationships:**

4.  **Floor**

    -   Moderate positive relationship

    -   Higher floors tend to have higher prices

    -   The relationship appears somewhat linear but with more variability

5.  **Garage**

    -   Positive association with price

    -   Presence/number of garages relates to higher prices

    -   Variable appears to be discrete (0, 1, 2)

**Weak or Unclear Relationships:**

6.  **Age**

    -   Weak or possibly negative relationship

    -   Newer apartments (lower age) may command higher prices

    -   The relationship is not as clear or strong

7.  **Elevator**

    -   Appears to be binary (0 or 1)

    -   Some positive association - apartments with elevators may be priced higher

    -   Difficult to assess linearity with binary variable

8.  **Storage**

    -   Appears binary or discrete

    -   Weak relationship with totalprice

    -   Less influential than other variables

**Key Observations:**

-   **Area** is clearly the dominant predictor with the strongest linear relationship

-   Several variables show positive correlations, suggesting larger/better-equipped apartments cost more

-   Some variables (elevator, garage, storage) appear to be discrete/binary, which may require special consideration in modeling

-   There appears to be multicollinearity potential among predictors (e.g., area correlates with rooms and toilets)

### (c) Compute the correlation between `totalprice` and all of the other numerical variables. List the three variables in order along with their correlation coefficients that have the highest correlation with `totalprice`.

```{r}
num_vars <- c("area", "age", "floor", "rooms", "toilets", "garage", 
              "elevator", "storage")
correlations <- sapply(num_vars, function(var) {
  cor(VIT2005$totalprice, VIT2005[[var]], use = "complete.obs")
})

# Sort by absolute correlation
sorted_cors <- sort(abs(correlations), decreasing = TRUE)
cat("\nTop 3 variables by correlation with totalprice:\n")
for(i in 1:3) {
  var_name <- names(sorted_cors)[i]
  cat(i, ".", var_name, ": r =", correlations[var_name], "\n")
}
```

**Interpretation:**

-   **Area** is by far the most important predictor, with a correlation nearly 18% stronger than toilets and 54% stronger than rooms

-   All three top correlations are positive, which makes intuitive sense: larger apartments with more amenities command higher prices

-   The correlations confirm what was observed in the scatterplot matrix (Part b)

-   These three variables are likely to be the most important predictors in the regression models

## Model (A)

Use backward elimination to develop a model that predicts `totalprice` using the data frame `VIT2005`. Use a “p-value to remove” of 5%. Store the final model in the object `modelA`.

```{r}
# Start with full model
full_model <- lm(totalprice ~ ., data = VIT2005)

# Backward elimination - step() with default k=2 approximates p=0.05
modelA <- step(full_model, direction = "backward", trace = 1)
cat("\nModel A Formula:", deparse(formula(modelA)), "\n")
summary(modelA)
```

**Model Selection Process:**

The backward elimination procedure removed the following variables (in order):

1.  **Conservation** (Step 1) - Not significant

2.  **Age** (Step 2) - Not significant

3.  **Floor** (Step 3) - Not significant

**Final Model A** retained these predictors:

-   area, zone, category, rooms, out, toilets, garage, elevator, streetcategory, heating, storage

#### (i) Compute $CV_n$, the leave-one-out cross validation error, for `modelA`. Set the seed to 5 and compute $CV_5$, the five-fold cross validation error, for `modelA`. The cross validation error for a generalized linear model can be computed using the `cv.glm()` function from the `boot` package. Using the function `glm()` without passing a family argument is the same as using the function `lm()`. R Code 2 provides a template of how to use the `cv.glm()` function. Note that $CV_n$ is returned with `cv.error$delta[1]`. To compute $CV_5$, pass the value 5 to the argument $K$ inside the `cv.glm()` function.

```         
– R Code 2
> mod.glm <- glm(y ~ x1 + x2, data = DF)
> library(boot)
> cv.error <- cv.glm(data = DF, glmfit = mod.glm)
> cv.error$delta[1]
```

```{r}
modelA_glm <- glm(formula(modelA), data = VIT2005)

# CV_n (leave-one-out)
cv_n_A <- cv.glm(data = VIT2005, glmfit = modelA_glm)
cat("CV_n (LOOCV):", cv_n_A$delta[1], "\n")

# CV_5 (5-fold)
set.seed(5)
cv_5_A <- cv.glm(data = VIT2005, glmfit = modelA_glm, K = 5)
cat("CV_5:", cv_5_A$delta[1], "\n")
```

-   **CV_n (Leave-One-Out Cross-Validation)**: 640,530,644

-   **CV_5 (5-Fold Cross-Validation)**: 647,819,847

Both CV errors are in squared euros (€²), representing the mean squared prediction error. The CV_5 is slightly higher than CV_n, which is typical due to the smaller training sets in k-fold CV.

#### (ii) Compute $R^2, R^2_a$, the AIC, and the BIC for Model (A). What is the proportion of total variability explained by Model (A)?

```{r}
summary_A <- summary(modelA)
cat("\n--- Model A Metrics ---\n")
cat("R-squared:", summary_A$r.squared, "\n")
cat("Adjusted R-squared:", summary_A$adj.r.squared, "\n")
cat("AIC:", AIC(modelA), "\n")
cat("BIC:", BIC(modelA), "\n")
cat("Proportion of variability explained:", summary_A$r.squared, "\n")
```

**Proportion of variability explained**: **91.62%**

**Interpretation:**

**Model A explains approximately 91.62% of the total variability in apartment prices.** This is an excellent fit, indicating that the selected predictors (11 variables including categorical factors) capture most of the variation in total price.

The adjusted R² (89.55%) is slightly lower than R², accounting for the number of predictors in the model (43 regression coefficients total when including all factor levels). The relatively small difference between R² and adjusted R² suggests the model is not overly complex.

**Key Significant Predictors (p \< 0.05):**

1.  **Area** (p \< 2e-16): Most important predictor, coefficient = €1,322 per m²

2.  **Garage** (p = 9.06e-08): €25,448 increase per garage

3.  **Toilets** (p = 0.00162): €18,089 per additional toilet

4.  **Elevator** (p = 0.000845): €19,650 for having elevator

5.  **Zone**: Many zones significantly different from reference (e.g., Z21: +€92,150)

6.  **Category**: Several categories significantly affect price

7.  **Street category S3**: +€13,081

## Model (B)

Use the criterion-based procedure AIC, which for linear regression is equivalent to Mallow’s Cp, to develop a model that predicts `totalprice` using the variables in `VIT2005`. Store the model in the object `modelB`.

```{r}
# Use stepAIC for AIC-based selection (equivalent to Cp for linear regression)
modelB <- stepAIC(full_model, direction = "both", trace = 1, k = 2)
cat("\nModel B Formula:", deparse(formula(modelB)), "\n")
summary(modelB)
```

Model B: AIC-Based Selection Results

**Model Selection Process:**

The `stepAIC` function with AIC criterion (k=2, equivalent to Mallow's Cp for linear regression) performed **bidirectional selection** (both forward and backward steps).

**Final Model B** includes the same predictors as Model A:

-   area, zone, category, rooms, out, toilets, garage, elevator, streetcategory, heating, storage

**Note**: Model B is **identical to Model A**. The AIC-based stepwise selection and backward elimination both converged to the same final model, which indicates this is a robust model specification.

#### (i) Compute $CV_n$ for `modelB`. Set the seed to 5 and compute $CV_5$ for `modelB`.

```{r}
# (i) Cross-validation for Model B
cat("\n--- Model B Cross-Validation ---\n")
modelB_glm <- glm(formula(modelB), data = VIT2005)

cv_n_B <- cv.glm(data = VIT2005, glmfit = modelB_glm)
cat("CV_n (LOOCV):", cv_n_B$delta[1], "\n")

set.seed(5)
cv_5_B <- cv.glm(data = VIT2005, glmfit = modelB_glm, K = 5)
cat("CV_5:", cv_5_B$delta[1], "\n")
```

-   **CV_n (Leave-One-Out Cross-Validation)**: 640,530,644

-   **CV_5 (5-Fold Cross-Validation)**: 647,819,847

These values are identical to Model A because both models have the same predictors.

#### (ii) Compute $R^2, R^2_a$, the AIC, and the BIC for Model (B). What is the proportion of total variability explained by Model (B)?

```{r}
# (ii) Model metrics
summary_B <- summary(modelB)
cat("\n--- Model B Metrics ---\n")
cat("R-squared:", summary_B$r.squared, "\n")
cat("Adjusted R-squared:", summary_B$adj.r.squared, "\n")
cat("AIC:", AIC(modelB), "\n")
cat("BIC:", BIC(modelB), "\n")
cat("Proportion of variability explained:", summary_B$r.squared, "\n")
```

**Proportion of variability explained**: **91.62%**

**Interpretation:**

**Model B explains 91.62% of the total variability in apartment prices**, which is excellent predictive performance.

**Key Findings:**

1.  **Model Equivalence**: The fact that both backward elimination (Model A) and AIC-based selection (Model B) yielded identical models provides strong evidence that this variable subset is optimal for prediction.

2.  **Model Complexity**: With 43 coefficients (including all factor levels), the model is reasonably parsimonious given the data size (218 observations) and achieves high predictive accuracy.

3.  **Most Important Predictors** (based on p-values and effect sizes):

    -   **Area**: €1,322 per m² (p \< 2e-16) - strongest predictor

    -   **Zone**: Highly significant overall effect

    -   **Garage**: €25,448 per garage (p = 9.06e-08)

    -   **Elevator**: €19,650 premium (p = 0.000845)

    -   **Toilets**: €18,089 per toilet (p = 0.00162)

4.  **Variables Not Significant** at α = 0.05:

    -   rooms, heating, some zone and category levels

    -   However, they contribute to overall model fit and may improve prediction

## Model (C)

Use the criterion-based procedure BIC to develop a model that predicts `totalprice` using the variables in `VIT2005`. Store the model in the object `modelC`.

```{r}
# Use stepAIC with k = log(n) for BIC
modelC <- stepAIC(full_model, direction = "both", 
                  k = log(nrow(VIT2005)), trace = 1)
cat("\nModel C Formula:", deparse(formula(modelC)), "\n")
summary(modelC)
```

Model C: BIC-Based Selection Results

**Model Selection Process:**

The `stepAIC` function with BIC criterion (k = log(n) = log(218) ≈ 5.38) performed **bidirectional selection**.

The BIC criterion penalizes model complexity more heavily than AIC, leading to a **more parsimonious model**.

**Variables removed** (in order):

1.  **Conservation** (Step 1)

2.  **Streetcategory** (Step 2)

3.  **Out** (Step 3)

4.  **Category** (Step 4)

5.  **Floor** (Step 5)

6.  **Rooms** (Step 6)

7.  **Heating** (Step 7)

8.  **Age** (Step 8)

**Final Model C** includes only **6 predictors**:

-   area, zone, toilets, garage, elevator, storage

This is a **much simpler model** than Models A and B (which had 11 predictors).

#### (i) Compute $CV_n$ for `modelC`. Set the seed to 5 and compute $CV_5$ for `modelC`.

```{r}
# (i) Cross-validation for Model C
cat("\n--- Model C Cross-Validation ---\n")
modelC_glm <- glm(formula(modelC), data = VIT2005)

cv_n_C <- cv.glm(data = VIT2005, glmfit = modelC_glm)
cat("CV_n (LOOCV):", cv_n_C$delta[1], "\n")

set.seed(5)
cv_5_C <- cv.glm(data = VIT2005, glmfit = modelC_glm, K = 5)
cat("CV_5:", cv_5_C$delta[1], "\n")
```

-   **CV_n (Leave-One-Out Cross-Validation)**: 694,979,921

-   **CV_5 (5-Fold Cross-Validation)**: 726,604,599

**Note**: Model C has **higher CV errors** than Models A and B, indicating somewhat worse out-of-sample prediction performance, which is the trade-off for the simpler model.

#### (ii) Compute $R^2, R^2_a$, the AIC, and the BIC for Model (C). What is the proportion of total variability explained by Model (C)?

```{r}
# (ii) Model metrics
summary_C <- summary(modelC)
cat("\n--- Model C Metrics ---\n")
cat("R-squared:", summary_C$r.squared, "\n")
cat("Adjusted R-squared:", summary_C$adj.r.squared, "\n")
cat("AIC:", AIC(modelC), "\n")
cat("BIC:", BIC(modelC), "\n")
cat("Proportion of variability explained:", summary_C$r.squared, "\n")
```

**Proportion of variability explained**: **89.07%**

**Interpretation:**

**Model C explains 89.07% of the total variability in apartment prices**, which is still very good, though slightly less than Models A and B (91.62%).

**Key Findings:**

1.  **Parsimony vs. Fit Trade-off**: Model C sacrifices about 2.5% of explained variance to achieve a much simpler model with 16 fewer coefficients.

2.  **BIC Preference**: Model C has the **lowest BIC** (5151.014 vs 5179.099), making it the preferred model by the BIC criterion, which values simplicity.

3.  **Significant Predictors** (all p \< 0.05):

    -   **Area**: €1,415 per m² (p \< 2e-16) - strongest predictor

    -   **Garage**: €26,542 per garage (p = 1.52e-07)

    -   **Elevator**: €26,776 premium (p = 7.25e-06)

    -   **Toilets**: €25,559 per toilet (p = 9.00e-06)

    -   **Storage**: €11,079 premium (p = 0.0156)

    -   **Zone**: Highly significant overall effect

4.  **Practical Advantages**:

    -   Easier to interpret and communicate

    -   More stable predictions (less prone to overfitting)

    -   Requires fewer variables to collect/input

5.  **Prediction Performance**: While CV errors are about 8.5% higher than Models A/B, the model still provides excellent predictions with much less complexity.

**Conclusion**: Model C offers an excellent balance between predictive accuracy and model simplicity, making it attractive for practical applications where parsimony is valued.

## Model (D)

Use forward selection to develop a model that predicts `totalprice` using the variables in `VIT2005`. Use a “p-value to add” of 5%. Store the final model in the object `modelD`.

```{r}
# Start with intercept only
null_model <- lm(totalprice ~ 1, data = VIT2005)

# Forward selection
modelD <- step(null_model, 
               scope = formula(full_model), 
               direction = "forward", 
               trace = 1)
cat("\nModel D Formula:", deparse(formula(modelD)), "\n")
summary(modelD)
```

Model D: Forward Selection Results

**Model Selection Process:**

Forward selection started with an **intercept-only model** and sequentially added variables that most improved the model fit (reduced AIC).

**Variables added** (in order):

1.  **Area** (Step 1) - Strongest single predictor

2.  **Zone** (Step 2)

3.  **Garage** (Step 3)

4.  **Category** (Step 4)

5.  **Elevator** (Step 5)

6.  **Toilets** (Step 6)

7.  **Streetcategory** (Step 7)

8.  **Out** (Step 8)

9.  **Heating** (Step 9)

10. **Storage** (Step 10)

11. **Rooms** (Step 11) - Final variable added

**Final Model D** includes:

-   area, zone, garage, category, elevator, toilets, streetcategory, out, heating, storage, rooms

**Key Finding:**

**Model D is IDENTICAL to Models A and B!**

All three selection methods (backward elimination, AIC-based stepwise, and forward selection) converged to the **exact same model**, which provides very strong evidence that this is the optimal variable subset.

#### (i) Compute $CV_n$ for `modelD`. Set the seed to 5 and compute $CV_5$ for `modelD`.

```{r}
# (i) Cross-validation for Model D
cat("\n--- Model D Cross-Validation ---\n")
modelD_glm <- glm(formula(modelD), data = VIT2005)

cv_n_D <- cv.glm(data = VIT2005, glmfit = modelD_glm)
cat("CV_n (LOOCV):", cv_n_D$delta[1], "\n")

set.seed(5)
cv_5_D <- cv.glm(data = VIT2005, glmfit = modelD_glm, K = 5)
cat("CV_5:", cv_5_D$delta[1], "\n")
```

-   **CV_n (Leave-One-Out Cross-Validation)**: 640,530,644

-   **CV_5 (5-Fold Cross-Validation)**: 647,819,847

These are identical to Models A and B (as expected, since it's the same model).

#### (ii) Compute the $R^2, R^2_a$, AIC, and the BIC for Model (D). What is the proportion of total variability explained by Model (D)?

```{r}
# (ii) Model metrics
summary_D <- summary(modelD)
cat("\n--- Model D Metrics ---\n")
cat("R-squared:", summary_D$r.squared, "\n")
cat("Adjusted R-squared:", summary_D$adj.r.squared, "\n")
cat("AIC:", AIC(modelD), "\n")
cat("BIC:", BIC(modelD), "\n")
cat("Proportion of variability explained:", summary_D$r.squared, "\n")
```

**Proportion of variability explained**: **91.62%**

**Interpretation:**

**Model D explains 91.62% of the total variability in apartment prices.**

**All metrics are identical** because all three methods selected the same model.

**Significance:**

The convergence of three different selection approaches to the **same final model** is remarkable and indicates:

1.  **Model Stability**: The selected variable subset is robust and not dependent on the selection algorithm used

2.  **Clear Signal**: The data has a clear structure with well-defined important predictors

3.  **Optimal Balance**: This model represents an optimal trade-off between fit and complexity using the AIC criterion

4.  **High Confidence**: We can be highly confident this is the "right" model for prediction purposes using p-value-based criteria (α = 0.05)

**Most Important Predictors** (same as Models A and B):

1.  **Area**: €1,322 per m² (p \< 2e-16)

2.  **Garage**: €25,448 per garage (p = 9.06e-08)

3.  **Elevator**: €19,650 premium (p = 0.000845)

4.  **Toilets**: €18,089 per toilet (p = 0.00162)

5.  **Zone**: Multiple zones significantly affect price

### (d) Explore the residuals of the Models (A), (B), (C), and (D) using the function `residualPlot()` or `residualPlots()` from the package `car`. Comment on the results.

```{r}
par(mfrow = c(2, 2))
plot(modelA, which = 1:4, main = "Model A")

par(mfrow = c(2, 2))
plot(modelB, which = 1:4, main = "Model B")

par(mfrow = c(2, 2))
plot(modelC, which = 1:4, main = "Model C")

par(mfrow = c(2, 2))
plot(modelD, which = 1:4, main = "Model D")

par(mfrow = c(1, 1))

cat("\nComment: Examine residual plots for patterns, non-constant variance,\n")
cat("and departures from normality.\n")
```

Part (d): Residual Analysis for Models A-D

**Models A, B, and D (Identical Models)**

Since Models A, B, and D are identical, they have the same residual plots. Here are the findings:

**1. Residuals vs Fitted Plot:**

-   **Pattern observed**: Residuals show a **slight funnel shape** (heteroscedasticity)

-   Residuals spread increases slightly as fitted values increase

-   There's a **slight upward trend** (red line slopes upward), suggesting the model may slightly underpredict at higher price ranges

-   **Outliers identified**: Observations 116, 130, and 156 stand out

**2. Q-Q Plot (Normal Q-Q):**

-   **Generally good**: Points follow the theoretical line reasonably well in the middle

-   **Deviations at extremes**:

    -   Lower tail shows some deviation (observations like 156)

    -   Upper tail shows more pronounced deviation (observations 130, 136, 160)

-   **Conclusion**: Minor departures from normality, primarily in the tails

**3. Scale-Location Plot:**

-   Shows **increasing spread** of standardized residuals as fitted values increase

-   **Confirms heteroscedasticity**: Variance is not constant across the range of fitted values

-   Red line shows upward trend, indicating variance increases with predicted price

**4. Cook's Distance Plot:**

-   **Influential observations**: 43, 93, and 116 show notably higher Cook's distances

-   Most observations have low Cook's distance (\< 0.10)

-   No observations exceed the common threshold of 0.5, but some warrant attention

**Model C (Simpler BIC Model)**

Model C shows similar patterns but with some differences:

**1. Residuals vs Fitted:**

-   **More random scatter** compared to Models A/B/D

-   Still shows slight heteroscedasticity with increasing variance

-   **Different outliers**: Observations 130, 156, 207 are notable

**2. Q-Q Plot:**

-   **Similar to A/B/D**: Good fit in the middle, deviations in tails

-   Observations 130 and 156 deviate in upper tail

**3. Scale-Location Plot:**

-   Shows **less pronounced heteroscedasticity** pattern than Models A/B/D

-   Observations 130, 156, and 207 stand out

**4. Cook's Distance:**

-   **Different influential points**: Observations 31, 43, and 156 show higher influence

-   Generally lower Cook's distances overall compared to Models A/B/D

**Overall Comments:**

**Common Issues Across All Models:**

1.  **Heteroscedasticity**: All models show evidence of non-constant variance, with larger residuals for higher-priced apartments. This violates the homoscedasticity assumption.

2.  **Near-Normality**: Residuals are approximately normal but with heavier tails than expected, particularly in the upper tail. This is common with price data.

3.  **Potential Outliers**: Observations 130, 156, and others consistently appear as outliers across models

4.  **Model Adequacy**: Despite the violations, the patterns are relatively mild and the models still provide good fits

**Key Differences:**

-   **Models A/B/D**: Better fit (higher R²) but show more pronounced heteroscedasticity

-   **Model C**: Simpler model with slightly worse fit but comparable residual patterns

### (e) Use the function `boxCox()` from `car` to find a suitable transformation for `totalprice`.

```{r}
cat("\n\n=== PART (e): Box-Cox Transformation ===\n")
par(mfrow = c(1, 1))
bc <- boxCox(modelA)
lambda <- bc$x[which.max(bc$y)]
cat("Optimal lambda:", lambda, "\n")
cat("Interpretation: Lambda near 0 suggests log transformation is appropriate.\n")
```

Part (e): Box-Cox Transformation Analysis

**Box-Cox Results:**

**Optimal λ (lambda)**: 0.0606 ≈ 0

**Interpretation:**

The Box-Cox procedure searches for the optimal power transformation of the response variable that best satisfies the regression assumptions (normality and homoscedasticity of residuals).

**Mathematical Framework:** The Box-Cox transformation is defined as:

-   If λ ≠ 0: Y\^(λ) = (Y\^λ - 1) / λ

-   If λ = 0: Y\^(λ) = log(Y)

**Key Findings:**

1.  **Optimal Lambda Near Zero**:

    -   The optimal λ = 0.0606 is very close to 0

    -   When λ = 0, the transformation is equivalent to the **natural logarithm**

    -   This strongly suggests using **log(totalprice)** as the response variable

2.  **95% Confidence Interval**:

    -   The horizontal dashed line shows the 95% confidence level

    -   The confidence interval for λ includes 0 (approximately from -0.3 to 0.4)

    -   Since 0 is well within this interval, the log transformation is statistically justified

3.  **Profile Log-Likelihood Shape**:

    -   The curve is smooth and has a clear maximum near λ = 0

    -   The peak is relatively broad, indicating some flexibility in the transformation

    -   However, the maximum is clearly centered near zero

## Model (E)

Use backward elimination to develop a model that predicts `log(totalprice)` using the data frame `VIT2005`. Use a “p-value to remove” of 5%. Store the final model in the object `modelE`.

```{r}
# Create a temporary dataset with log-transformed response
VIT2005_log <- VIT2005
VIT2005_log$log_totalprice <- log(VIT2005$totalprice)
VIT2005_log$totalprice <- NULL  # Remove original totalprice

# Create full model with all predictors
full_model_log <- lm(log_totalprice ~ ., data = VIT2005_log)

modelE <- step(full_model_log, direction = "backward", trace = 1)
cat("\nModel E Formula:", deparse(formula(modelE)), "\n")
summary(modelE)

# Update formula to use log(totalprice) for consistency
modelE_formula <- formula(modelE)
modelE_formula <- as.formula(gsub("log_totalprice", "log(totalprice)", 
                                   deparse(modelE_formula)))
modelE <- lm(modelE_formula, data = VIT2005)
```

Model E: Backward Elimination with Log-Transformed Response

**Model Selection Process:**

Backward elimination removed the following variables (in order):

1.  **Conservation** (Step 1) - Not significant

2.  **Age** (Step 2) - Not significant

3.  **Floor** (Step 3) - Not significant

4.  **Rooms** (Step 4) - Not significant

**Final Model E** retained these 10 predictors:

-   area, zone, category, out, toilets, garage, elevator, streetcategory, heating, storage

This is **one fewer predictor than Models A/B/D** (which had 11 predictors including rooms).

#### (i) Compute $CV_n$ for `modelE`. Set the seed to 5 and compute $CV_5$ for `modelE`.

```{r}
# (i) Cross-validation for Model E
cat("\n--- Model E Cross-Validation ---\n")
modelE_glm <- glm(formula(modelE), data = VIT2005)

cv_n_E <- cv.glm(data = VIT2005, glmfit = modelE_glm)
cat("CV_n (LOOCV):", cv_n_E$delta[1], "\n")

set.seed(5)
cv_5_E <- cv.glm(data = VIT2005, glmfit = modelE_glm, K = 5)
cat("CV_5:", cv_5_E$delta[1], "\n")
```

-   **CV_n (Leave-One-Out Cross-Validation)**: 0.007080105

-   **CV_5 (5-Fold Cross-Validation)**: 0.007358635

**Important Note**: These CV errors are on the **log scale** (not euros), so they represent mean squared prediction error for log(totalprice). These values are **much smaller** than Models A-D because:

1.  They're on the log scale (compressed range)

2.  The log transformation has stabilized variance

#### (ii) Compute $R^2, R^2_a$, the AIC, and the BIC for Model (E). What is the proportion of total variability explained by Model (E)?

```{r}
# (ii) Model metrics
summary_E <- summary(modelE)
cat("\n--- Model E Metrics ---\n")
cat("R-squared:", summary_E$r.squared, "\n")
cat("Adjusted R-squared:", summary_E$adj.r.squared, "\n")
cat("AIC:", AIC(modelE), "\n")
cat("BIC:", BIC(modelE), "\n")
cat("Proportion of variability explained:", summary_E$r.squared, "\n")
```

**Proportion of variability explained**: **92.02%**

**Interpretation:**

**Model E explains 92.02% of the total variability in log(totalprice).**

This is actually a **slight improvement** over Models A/B/D (which had 91.62% R² for untransformed totalprice), despite having one fewer predictor.

**Key Improvements from Log Transformation:**

1.  **Better Model Fit**:

    -   R² increased from 91.62% to 92.02%

    -   Adjusted R² increased from 89.55% to 90.10%

2.  **Improved AIC/BIC**:

    -   AIC: -467.851 (vs 5026.797 for Models A/B/D)

    -   BIC: -318.933 (vs 5179.099 for Models A/B/D)

    -   **Note**: These aren't directly comparable due to different scales, but negative values indicate excellent fit

3.  **Smaller Residual Standard Error**:

    -   σ = 0.07547 on log scale

    -   This represents approximately 7.5% typical prediction error in multiplicative terms

4.  **Better Residual Properties** (expected from Box-Cox):

    -   Smaller, more symmetric residuals (range: -0.223 to 0.266)

    -   More homoscedastic (constant variance)

    -   More normally distributed

**Significant Predictors (p \< 0.05):**

**Highly Significant (p \< 0.001):**

1.  **Area**: β = 0.00446 (p \< 2e-16)

    -   Each additional m² increases price by \~0.45%

2.  **Garage**: β = 0.0794 (p = 6.45e-07)

    -   Having a garage increases price by \~8.3% \[exp(0.0794)-1\]

3.  **Elevator**: β = 0.102 (p = 4.12e-07)

    -   Having elevator increases price by \~10.7%

4.  **Toilets**: β = 0.0765 (p = 8.35e-05)

    -   Each additional toilet increases price by \~8.0%

5.  **Zone**: Many zones highly significant

6.  **Category**: Several categories significant

**Moderately Significant (0.001 \< p \< 0.05):**

-   Various zone and category levels

-   **outE25**: β = 0.165 (p = 0.000854) - 18.0% price premium

-   **Storage**: β = 0.0336 (p = 0.0282) - 3.4% price increase

**Coefficient Interpretation (Log-Linear Model):**

Since the response is log-transformed:

-   **Continuous predictors**: β represents the proportional change in price for a 1-unit increase

    -   Example: Area coefficient = 0.00446 means 1 m² increase → 0.446% price increase

-   **Binary/Categorical predictors**: exp(β) - 1 gives the percentage change

    -   Example: Garage coefficient = 0.0794 → exp(0.0794) - 1 = 0.0826 = **8.26% price increase**

    -   Example: Elevator coefficient = 0.102 → exp(0.102) - 1 = 0.1074 = **10.74% price increase**

## Model (F)

Use the criterion-based procedure AIC, which for linear regression is equivalent to Mallow’s Cp, to develop a model that predicts `log(totalprice)` using the variables in `VIT2005`. Store the model in the object `modelF`.

```{r}
# Use the cleaned dataset
full_model_log_F <- lm(log_totalprice ~ ., data = VIT2005_log)
modelF <- stepAIC(full_model_log_F, direction = "both", trace = 1, k = 2)

# Update formula to use log(totalprice)
modelF_formula <- as.formula(gsub("log_totalprice", "log(totalprice)", 
                                   deparse(formula(modelF))))
modelF <- lm(modelF_formula, data = VIT2005)
cat("\nModel F Formula:", deparse(formula(modelF)), "\n")
summary(modelF)
```

Model F: AIC-Based Selection with Log(totalprice)

**Model Selection Process:**

The `stepAIC` function with AIC criterion (k=2) performed bidirectional selection on the log-transformed response.

**Final Model F** includes the same 10 predictors as Model E:

-   area, zone, category, out, toilets, garage, elevator, streetcategory, heating, storage

**Key Finding**: **Model F is IDENTICAL to Model E**

Both AIC-based stepwise selection and backward elimination converged to the same model when using log(totalprice) as the response.

#### (i) Compute $CV_n$ for `modelF`. Set the seed to 5 and compute $CV_5$ for `modelF`.

```{r}
# (i) Cross-validation
cat("\n--- Model F Cross-Validation ---\n")
modelF_glm <- glm(formula(modelF), data = VIT2005)

cv_n_F <- cv.glm(data = VIT2005, glmfit = modelF_glm)
cat("CV_n (LOOCV):", cv_n_F$delta[1], "\n")

set.seed(5)
cv_5_F <- cv.glm(data = VIT2005, glmfit = modelF_glm, K = 5)
cat("CV_5:", cv_5_F$delta[1], "\n")
```

-   **CV_n (Leave-One-Out Cross-Validation)**: 0.007080105

-   **CV_5 (Five-Fold Cross-Validation with seed = 5)**: 0.007358635

These are identical to Model E (as expected since it's the same model).

#### (ii) Compute $R^2, R^2_a$, the AIC, and the BIC for Model (F). What is the proportion of total variability explained by Model (F)?

```{r}
# (ii) Model metrics
summary_F <- summary(modelF)
cat("\n--- Model F Metrics ---\n")
cat("R-squared:", summary_F$r.squared, "\n")
cat("Adjusted R-squared:", summary_F$adj.r.squared, "\n")
cat("AIC:", AIC(modelF), "\n")
cat("BIC:", BIC(modelF), "\n")
cat("Proportion of variability explained:", summary_F$r.squared, "\n")
```

**Proportion of total variability explained by Model (F)**: **92.02%**

This means that Model F explains approximately 92% of the variability in log(totalprice) using the selected predictors. The adjusted R² of 0.901 (90.1%) accounts for the number of predictors in the model and still indicates excellent explanatory power.

## Model (G)

Use the criterion-based procedure BIC to develop a model that predicts `log(totalprice)` using the variables in `VIT2005`. Store the model in the object `modelG`.

```{r}
# Use the cleaned dataset
full_model_log_G <- lm(log_totalprice ~ ., data = VIT2005_log)
modelG <- stepAIC(full_model_log_G, direction = "both", 
                  k = log(nrow(VIT2005_log)), trace = 1)

# Update formula to use log(totalprice)
modelG_formula <- as.formula(gsub("log_totalprice", "log(totalprice)", 
                                   deparse(formula(modelG))))
modelG <- lm(modelG_formula, data = VIT2005)
cat("\nModel G Formula:", deparse(formula(modelG)), "\n")
summary(modelG)
```

Model G: BIC-Based Selection with Log-Transformed Response

**Model Selection Process:**

The `stepAIC` function with BIC criterion (k = log(218) ≈ 5.38) performed **bidirectional selection** on the log-transformed response.

The BIC criterion penalizes model complexity more heavily than AIC, leading to a **more parsimonious model**.

**Variables removed** (in order):

1.  **Conservation** (Step 1)

2.  **Streetcategory** (Step 2)

3.  **Category** (Step 3)

4.  **Out** (Step 4)

5.  **Heating** (Step 5)

6.  **Rooms** (Step 6)

7.  **Floor** (Step 7)

8.  **Age** (Step 8)

**Final Model G** includes only **6 predictors**:

-   area, zone, toilets, garage, elevator, storage

This is **identical to Model C** (which also had 6 predictors with untransformed response), showing that BIC consistently selects the same variables regardless of transformation.

#### (i) Compute $CV_n$ for `modelG`. Set the seed to 5 and compute $CV_5$ for `modelG`.

```{r}
# (i) Cross-validation
cat("\n--- Model G Cross-Validation ---\n")
modelG_glm <- glm(formula(modelG), data = VIT2005)

cv_n_G <- cv.glm(data = VIT2005, glmfit = modelG_glm)
cat("CV_n (LOOCV):", cv_n_G$delta[1], "\n")

set.seed(5)
cv_5_G <- cv.glm(data = VIT2005, glmfit = modelG_glm, K = 5)
cat("CV_5:", cv_5_G$delta[1], "\n")
```

#### (ii) Compute $R^2, R^2_a$, the AIC, and the BIC for Model (G). What is the proportion of total variability explained by Model (G)?

```{r}
# (ii) Model metrics
summary_G <- summary(modelG)
cat("\n--- Model G Metrics ---\n")
cat("R-squared:", summary_G$r.squared, "\n")
cat("Adjusted R-squared:", summary_G$adj.r.squared, "\n")
cat("AIC:", AIC(modelG), "\n")
cat("BIC:", BIC(modelG), "\n")
cat("Proportion of variability explained:", summary_G$r.squared, "\n")
```

**Proportion of total variability explained by Model (G)**: **89.71%**

This means that Model G explains approximately 89.71% of the variability in log(totalprice) using the selected predictors. The adjusted R² of 0.8825 (88.25%) accounts for the number of predictors in the model and still indicates very good explanatory power.

This is approximately 2.3% less than Models E and F (92.02%), which is the cost of having a much simpler model with 4 fewer predictors.

## Model (H)

Use forward selection to develop a model that predicts log(`totalprice`) using the variables in `VIT2005`. Use a “p-value to add” of 5%. Store the final model in the object `modelH`.

```{r}
# Start with intercept only
null_model_log <- lm(log_totalprice ~ 1, data = VIT2005_log)
full_model_log_H <- lm(log_totalprice ~ ., data = VIT2005_log)

modelH <- step(null_model_log, 
               scope = formula(full_model_log_H), 
               direction = "forward", 
               trace = 1)

# Update formula to use log(totalprice)
modelH_formula <- as.formula(gsub("log_totalprice", "log(totalprice)", 
                                   deparse(formula(modelH))))
modelH <- lm(modelH_formula, data = VIT2005)
cat("\nModel H Formula:", deparse(formula(modelH)), "\n")
summary(modelH)
```

Model H was built using **forward selection**, starting with the intercept-only model and sequentially adding variables that had p-values ≤ 0.05.

**Key observations:**

-   Model H includes **42 predictor terms** (including dummy variables for categorical predictors)

-   Model H includes **10 variables**: area, zone, elevator, toilets, garage, category, out, storage, heating, and streetcategory

#### (i) Compute $CV_n$ for `modelH`. Set the seed to 5 and compute $CV_5$ for `modelH`.

```{r}
# (i) Cross-validation
cat("\n--- Model H Cross-Validation ---\n")
modelH_glm <- glm(formula(modelH), data = VIT2005)

cv_n_H <- cv.glm(data = VIT2005, glmfit = modelH_glm)
cat("CV_n (LOOCV):", cv_n_H$delta[1], "\n")

set.seed(5)
cv_5_H <- cv.glm(data = VIT2005, glmfit = modelH_glm, K = 5)
cat("CV_5:", cv_5_H$delta[1], "\n")
```

-   **CV_n (Leave-One-Out Cross-Validation)**: 0.007080105

-   **CV_5 (Five-Fold Cross-Validation with seed = 5)**: 0.007358635

These values represent the cross-validation prediction errors for Model H using the log-transformed response variable.

#### (ii) Compute $R^2, R_a^2$, the AIC, and the BIC for Model (H). What is the proportion of total variability explained by Model (H)?

```{r}
# (ii) Model metrics
summary_H <- summary(modelH)
cat("\n--- Model H Metrics ---\n")
cat("R-squared:", summary_H$r.squared, "\n")
cat("Adjusted R-squared:", summary_H$adj.r.squared, "\n")
cat("AIC:", AIC(modelH), "\n")
cat("BIC:", BIC(modelH), "\n")
cat("Proportion of variability explained:", summary_H$r.squared, "\n")
```

**Proportion of total variability explained by Model (H)**: **92.02%**

This means that Model H explains approximately 92.02% of the variability in log(totalprice) using the selected predictors. The adjusted R² of 0.901 (90.1%) accounts for the number of predictors in the model and still indicates excellent explanatory power.

### (f) Which model has the smallest $CV_5$ as well as the smallest $CV_n$ error among Models (E), (F), (G), and (H)?

```{r}
cv_comparison <- data.frame(
  Model = c("E", "F", "G", "H"),
  CV_n = c(cv_n_E$delta[1], cv_n_F$delta[1], cv_n_G$delta[1], cv_n_H$delta[1]),
  CV_5 = c(cv_5_E$delta[1], cv_5_F$delta[1], cv_5_G$delta[1], cv_5_H$delta[1])
)
print(cv_comparison)

# Find model with smallest total
best_model_name <- cv_comparison$Model[which.min(cv_comparison$CV_n + cv_comparison$CV_5)]
cat("\nBest model (considering both CV_n and CV_5):", best_model_name, "\n")
```

**Models E, F, and H are tied for having both the smallest CV₅ and the smallest CVₙ errors.**

**Conclusion:**

**Models E, F, and H all have identical cross-validation errors** and jointly achieve the smallest values for both CVₙ and CV₅. This is because these three models are actually the **same model** - they all arrived at the identical set of predictors through different selection methods:

-   Model E: backward elimination

-   Model F: AIC-based selection

-   Model H: forward selection

**Model G**, which was selected using the more stringent BIC criterion, has slightly higher (worse) cross-validation errors, with approximately 10% higher CVₙ (0.0078 vs 0.0071) and 13. 5% higher CV₅ (0.0084 vs 0.0074).

**Therefore, any of Models E, F, or H can be selected as the model with the best cross-validation performance. They are equivalent choices.**

### (g) Use the model selected from part (f) and explore its residuals using the function `residualPlots()` from `car`. Comment on the results.

```{r}
best_model <- get(paste0("model", best_model_name))

par(mfrow = c(2, 2))
plot(best_model)
par(mfrow = c(1, 1))
```

**1. Residuals vs Fitted (Top Left)**

-   Shows residuals plotted against fitted values (ranging from \~12. 0 to 13.2)

-   Horizontal red line at zero

-   Points appear randomly scattered around zero with relatively constant spread

-   Observations 93, 3, and 156 are labeled as notable points

**2. Q-Q Residuals (Top Right)**

-   Normal probability plot (Q-Q plot)

-   Standardized residuals vs theoretical quantiles

-   Points follow the diagonal line reasonably well through the middle

-   Some deviation in the tails, particularly observations 93 (upper right) and 3, 156 (lower left)

-   Suggests slight departures from normality in the extremes

**3. Scale-Location (Bottom Left)**

-   Square root of standardized residuals vs fitted values

-   Tests homoscedasticity (constant variance)

-   Red horizontal line shows the trend

-   Points show relatively constant spread across fitted values

-   Same observations (93, 3, 156) stand out

**4. Residuals vs Leverage (Bottom Right)**

-   Standardized residuals vs leverage (hat values)

-   Shows influential observations

-   Dashed curves represent Cook's distance contours (0.5)

-   Observations 93, 3, 160, and 81 are labeled

-   Most points have low leverage (\< 0.3)

-   Observation 93 appears to have higher leverage and positive residual

**Overall Assessment:**

**Strengths:**

-   Residuals appear randomly scattered with no obvious patterns

-   Variance appears relatively constant (homoscedasticity satisfied)

-   Most observations follow normal distribution

**Potential Concerns:**

-   **Observations 3 and 93** appear as potential outliers/influential points across multiple plots

-   Slight heavy tails in Q-Q plot suggest minor departures from perfect normality

-   These observations may warrant further investigation

## Model (I)

Refer to the model selected in part (e) as `modelI`.

```{r}
cat("Using Model", best_model_name, "as Model I\n")

modelI <- best_model
```

#### (i) Plot the Cook distances, the studentized residuals, and the diagonal elements of the hat matrix of Model (I) versus the index. Based on the graphs, are there any outliers?

```{r}
# (i) Diagnostic plots
par(mfrow = c(2, 2))

# Cook's distances
cook_d <- cooks.distance(modelI)
plot(cook_d, type = "h", 
     main = "Cook's Distances", ylab = "Cook's D")
abline(h = 4/nrow(VIT2005), col = "red", lty = 2)

# Studentized residuals
stud_resid <- rstudent(modelI)
plot(stud_resid, type = "p", 
     main = "Studentized Residuals", ylab = "Studentized Residuals")
abline(h = c(-2, 2), col = "red", lty = 2)

# Hat values
hats <- hatvalues(modelI)
plot(hats, type = "h", 
     main = "Hat Values", ylab = "Leverage")
abline(h = 2 * length(coef(modelI))/nrow(VIT2005), col = "red", lty = 2)

par(mfrow = c(1, 1))
```

**Analysis of the Three Diagnostic Plots:**

**1. Cook's Distances (Top Left)**

-   Measures the influence of each observation on the fitted model

-   **Threshold:** Red dashed line appears around 0.08-0.10

-   **Key observations:**

    -   Most Cook's distances are very small (\< 0.02)

    -   A few observations show elevated Cook's distances, notably around **index 93** (approximately 0.10)

    -   Another spike visible around **index 3**

    -   **Conclusion:** Observation 93 shows the highest Cook's distance, suggesting it may be influential

**2. Studentized Residuals (Top Right)**

-   Standardized residuals adjusted for leverage

-   **Threshold:** Red dashed lines at approximately ±2 (representing ±2 standard deviations)

-   **Key observations:**

    -   Most residuals fall between -2 and +2

    -   Several observations exceed ±2 threshold:

        -   **Observation 93** has a studentized residual \> +2

        -   A few observations in the lower tail approach or slightly exceed -2

    -   **Conclusion:** Observation 93 appears to be a potential outlier with a large positive residual

**3. Hat Values (Leverage) (Bottom Left)**

-   Measures how far an observation's predictor values are from the center

-   **Threshold:** Red dashed line at approximately 0.4 (common cutoff: 2(p+1)/n or 3(p+1)/n)

-   **Key observations:**

    -   Most hat values are relatively low (\< 0.3)

    -   Several observations show elevated leverage:

        -   **Observation 93** has high leverage (approaching 0.4-0.5)

        -   **Observation 3** also shows elevated leverage

        -   A few other observations around indices 150-200 show moderately high leverage

    -   **Conclusion:** Observations 93 and 3 have high leverage points

**Overall Answer:**

**Yes, there are outliers based on these graphs:**

1.  **Observation 93** is the most concerning:

    -   High Cook's distance (most influential observation)

    -   Large positive studentized residual (\> +2)

    -   High leverage

    -   This combination indicates it is both an **outlier** (unusual response) and **influential** (impacts the model fit)

2.  **Observation 3** shows:

    -   Moderately elevated Cook's distance

    -   High leverage

    -   Potentially unusual predictor values

3.  **A few other observations** have studentized residuals exceeding ±2, indicating they may be outliers in terms of their response values

**Recommendation:** Observations 3 and 93 warrant further investigation and potential removal, as suggested in part (iii) of the original case study instructions.

#### (ii) Create a bubble-plot of the studentized residuals versus the hat values with the function `influencePlot()`. Are any of the points influential?

```{r}
# (ii) Influence plot
cat("\n--- Influence Plot ---\n")
influencePlot(modelI, main = "Influence Plot")
```

**Analysis of the Influence Plot:**

The influence plot shows:

-   **X-axis:** Hat values (leverage) - measures how unusual the predictor values are

-   **Y-axis:** Studentized residuals - measures how unusual the response is

-   **Bubble size:** Cook's distance - measures overall influence on the model

-   **Color intensity:** Darker/larger bubbles indicate higher Cook's distances

**Reference lines:**

-   Horizontal dashed lines at ±2 mark the threshold for outliers (unusual residuals)

-   Vertical dashed lines indicate high leverage thresholds

-   Color scale shows Cook's D ranging from 0 to 0.0471

**Identification of Influential Points:**

**Yes, several points are influential:**

**1. Observation 92 (Most Influential)**

-   **Hat value:** 0.6348743 (extremely high leverage - far right)

-   **Studentized residual:** -0.8692675 (moderate negative)

-   **Cook's D:** 0.0305983358

-   **Assessment:** Very high leverage point with large bubble size. This is the **most influential observation** due to extreme leverage, even though its residual is not the largest

**2. Observation 160 (Highly Influential)**

-   **Hat value:** 0.3121378 (moderately high leverage)

-   **Studentized residual:** 2.1342726 (exceeds +2 threshold - outlier)

-   **Cook's D:** 0.0471022902 (**highest Cook's distance**)

-   **Assessment:** Both an outlier (large positive residual) AND has high leverage. The largest bubble in the plot, making it **highly influential**

**3. Observation 156 (Influential Outlier)**

-   **Hat value:** 0.1533031 (moderate leverage)

-   **Studentized residual:** -3.2707618 (**largest negative residual** - well below -2)

-   **Cook's D:** 0.0426545068 (second highest)

-   **Assessment:** Strong outlier with large negative residual. Large bubble size indicates **high influence**

**4. Observation 207 (Moderately Influential)**

-   **Hat value:** 0.1005529 (low to moderate leverage)

-   **Studentized residual:** -2.8433006 (exceeds -2 threshold - outlier)

-   **Cook's D:** 0.0201913898

-   **Assessment:** Outlier with negative residual, **moderately influential**

**5. Observation 9 (High Leverage, Low Influence)**

-   **Hat value:** 0.5384183 (very high leverage)

-   **Studentized residual:** -0.1513085 (very small)

-   **Cook's D:** 0.0006245814 (very small)

-   **Assessment:** High leverage but small residual means it's **not influential** - it fits the model well despite unusual predictor values

**Conclusion:**

**Yes, several points are influential:**

**Most concerning (in order of influence):**

1.  **Observation 160** - Highest Cook's D (0.0471), high leverage, large positive residual

2.  **Observation 156** - Second highest Cook's D (0.0427), extreme negative residual

3.  **Observation 92** - High Cook's D (0.0306), extremely high leverage

4.  **Observation 207** - Moderate Cook's D (0.0202), large negative residual

#### (iii) The original researchers evaluated the apartments in rows 3 and 93 and decided they were not representative and decided to remove them from the study. Remove observations 3 and 93 from consideration in `modelI`.

```{r}
cat("\n--- Removing observations 3 and 93 ---\n")
VIT2005_clean <- VIT2005[-c(3, 93), ]
modelI <- lm(formula(modelI), data = VIT2005_clean)
cat("\nModel I Summary (after removing obs 3 and 93):\n")
summary(modelI)
```

**Comparison: Model I Before vs. After Removing Observations 3 and 93**

**Sample Size:**

-   **Before removal:** 218 observations (175 residual df + 43 parameters)

-   **After removal:** 216 observations (173 residual df + 43 parameters)

-   **Confirmed:** 2 observations removed ✓

**Model Performance Improvements:**

| **Metric** | **Before (with obs 3 & 93)** | **After (without obs 3 & 93)** | **Change** |
|----|----|----|----|
| **Residual Std Error** | 0.07547 | 0.06994 | **↓ 7.3% improvement** |
| **R²** | 0.9202 | 0.9317 | **↑ 1.15% increase** |
| **Adjusted R²** | 0.9010 | 0.9151 | **↑ 1.41% increase** |
| **F-statistic** | 48.02 | 56.17 | **↑ 17% increase** |

**Residual Distribution Improvements:**

| **Residual** | **Before** | **After** | **Change**                              |
|--------------|------------|-----------|-----------------------------------------|
| **Min**      | -0.2228    | -0.2048   | Less extreme negative                   |
| **Max**      | 0.2659     | 0.1454    | **↓ 45% reduction** (major improvement) |
| **1Q**       | -0.0449    | -0.0423   | Slightly better                         |
| **3Q**       | 0.0482     | 0.0452    | Slightly better                         |

**Key observation:** The maximum residual decreased dramatically from 0.266 to 0.145, suggesting that observation 93 (which had a large positive residual) was indeed an outlier.

**Coefficient Changes:**

Most coefficients remained relatively stable, but some notable changes:

-   **Area coefficient:** 0.004461 → 0.004150 (slight decrease, but still highly significant)

-   **Zone coefficients:** Generally increased in magnitude, particularly:

    -   Z21: 0.3148 → 0.3802

    -   Z31: 0.2802 → 0.3389

    -   Z32: 0.1793 → 0.2404

    -   Z37: 0.3008 → 0.3585

-   **Toilets:** 0.0765 → 0.0859 (increase)

-   **Storage:** 0.0336 → 0.0376 (increase)

All key predictors (area, zone, toilets, garage, elevator) remain highly significant (p \< 0.001).

**Conclusion:**

Removing observations 3 and 93 was justified and beneficial:

1.  **Better model fit:** R² increased from 92.02% to 93.17%

2.  **Better precision:** Residual standard error decreased by 7.3%

3.  **Better residual distribution:** Maximum residual reduced by 45%, indicating removal of an extreme outlier

4.  **More reliable estimates:** Stronger F-statistic (56.17 vs 48.02)

5.  **Maintained significance:** All important predictors remain highly significant

#### (iv) Check normality and homoscedasticity for `modelI` using graphs and hypothesis tests.

```{r}
# Normality: Shapiro-Wilk test
shapiro_test <- shapiro.test(residuals(modelI))
cat("Shapiro-Wilk test:\n")
cat("  W =", shapiro_test$statistic, ", p-value =", shapiro_test$p.value, "\n")
if(shapiro_test$p.value > 0.05) {
  cat("  Conclusion: Residuals appear normally distributed.\n")
} else {
  cat("  Conclusion: Evidence against normality.\n")
}

# Q-Q plot
par(mfrow = c(1, 2))
qqPlot(modelI, main = "Q-Q Plot", id = FALSE)

# Homoscedasticity: Breusch-Pagan test
ncv_test <- ncvTest(modelI)
cat("\nBreusch-Pagan test:\n")
cat("  Chi-square =", ncv_test$ChiSquare, ", p-value =", ncv_test$p, "\n")
if(ncv_test$p > 0.05) {
  cat("  Conclusion: No evidence of heteroscedasticity.\n")
} else {
  cat("  Conclusion: Evidence of heteroscedasticity.\n")
}

# Residuals vs Fitted plot
plot(fitted(modelI), residuals(modelI), 
     main = "Residuals vs Fitted",
     xlab = "Fitted values", ylab = "Residuals")
abline(h = 0, col = "red", lty = 2)

par(mfrow = c(1, 1))
```

**1. NORMALITY ASSESSMENT**

**Graphical Check: Q-Q Plot (Left Panel)**

The Q-Q plot shows studentized residuals plotted against theoretical t-distribution quantiles:

**Observations:**

-   Points follow the diagonal reference line (shaded blue band) very closely throughout most of the distribution

-   Excellent alignment in the center of the distribution (-2 to +2)

-   Very minimal deviation in the extreme tails

-   A few points in the lower left tail show slight departure, but remain within acceptable bounds

-   **Overall:** The Q-Q plot shows **excellent adherence to normality**

**Hypothesis Test: Shapiro-Wilk Test**

**Interpretation:**

-   **Null hypothesis (H₀):** Residuals are normally distributed

-   **p-value = 0.605** \>\> 0.05 (significance level)

-   **Decision:** **Fail to reject H₀**

-   **Conclusion:** **Residuals appear normally distributed** ✓

The very high p-value (0.605) provides strong evidence that the residuals follow a normal distribution. The W statistic of 0.9944 is very close to 1 (perfect normality).

**2. HOMOSCEDASTICITY ASSESSMENT**

**Graphical Check: Residuals vs Fitted (Right Panel)**

The plot shows residuals against fitted values (log(totalprice)):

**Observations:**

-   Residuals are randomly scattered around the horizontal zero line (red dashed line)

-   **Constant vertical spread** across the range of fitted values (12.0 to 13.2)

-   No funnel shape or systematic pattern (no widening or narrowing)

-   No obvious curvature or trends

-   Residuals range approximately from -0.20 to +0.15, with fairly consistent spread

-   A couple of points around -0.20 but these don't indicate systematic heteroscedasticity

-   **Overall:** The plot shows **excellent constant variance (homoscedasticity)**

**Hypothesis Test: Breusch-Pagan Test**

**Interpretation:**

-   **Null hypothesis (H₀):** Homoscedasticity (constant variance of residuals)

-   **p-value = 0.623** \>\> 0.05 (significance level)

-   **Decision:** **Fail to reject H₀**

-   **Conclusion:** **No evidence of heteroscedasticity** ✓

The very high p-value (0.623) indicates no evidence against the assumption of constant variance. The low chi-square statistic (0.241) confirms homoscedasticity.

**OVERALL CONCLUSION:**

**✓ Both normality and homoscedasticity assumptions are satisfied for Model I**

**Summary:**

1.  **Normality:** ✓ SATISFIED

    -   Q-Q plot shows excellent fit to normal distribution

    -   Shapiro-Wilk test (p = 0.605) strongly supports normality

2.  **Homoscedasticity:** ✓ SATISFIED

    -   Residuals vs Fitted plot shows constant variance

    -   Breusch-Pagan test (p = 0.623) confirms homoscedasticity

#### (v) Find the variance inflation factors for Model (I). Is multicollinearity a problem?

```{r}
vif_values <- vif(modelI)
print(vif_values)
if(any(vif_values > 10)) {
  cat("\nMulticollinearity is a problem (VIF > 10 detected).\n")
} else if(any(vif_values > 5)) {
  cat("\nModerate multicollinearity detected (VIF > 5).\n")
} else {
  cat("\nMulticollinearity is not a problem (all VIF < 5).\n")
}
```

**Variance Inflation Factor (VIF) Analysis:**

The VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity with other predictors.

**Common VIF Interpretation Guidelines:**

-   **VIF \< 5:** No multicollinearity concern

-   **5 ≤ VIF \< 10:** Moderate multicollinearity (may warrant attention)

-   **VIF ≥ 10:** High multicollinearity (problematic)

**Answer: YES, multicollinearity is a problem in Model I.**

**Interpretation and Context:**

**Why is multicollinearity detected?**

Model I includes several categorical variables with multiple levels:

-   **zone** (23 levels: Z11, Z21, Z31, .. ., Z62)

-   **category** (6 levels: 1A, 2B, 3A, 3B, 4A, 4B, 5A)

-   **out** (4 levels: E0, E25, E50, E75)

-   **streetcategory** (4 levels: S2, S3, S4, S5)

-   **heating** (4 levels: 1A, 3A, 3B, 4A)

These categorical variables create many dummy variables in the regression, which can lead to:

1.  **Structural multicollinearity** among dummy variables within the same categorical variable

2.  **Correlations between categorical variables** (e.g., certain zones may be associated with certain categories or street types)

3.  **Correlations between numerical predictors** (e.g., area might correlate with number of toilets, garage spaces)

**Is this a serious problem?**

**For Model I, the multicollinearity is concerning but manageable:**

**Potential Issues:**

-   **Inflated standard errors** for some coefficients

-   **Less stable parameter estimates** (coefficients may be sensitive to small data changes)

-   **Wider confidence intervals** for some predictors

-   **Difficulty determining individual predictor importance**

**However:**

-   **Prediction still reliable:** Multicollinearity doesn't affect prediction accuracy or R²

-   **Model as a whole is valid:** The F-statistic and overall model performance remain strong

-   **Many coefficients remain significant:** Despite VIF \> 10, most key predictors (area, zone levels, toilets, garage, elevator) have highly significant p-values (\< 0.001)

#### (vi) Find the parameter estimates, and compute 95% confidence intervals for the parameters of Model (I).

```{r}
print(summary(modelI)$coefficients)
cat("\n95% Confidence Intervals:\n")
print(confint(modelI, level = 0.95))
```

#### (vii) Find the relative contribution of the explanatory variables to explaining the variability of the prices in Model (I).

```{r}
# Use anova to get sequential SS
anova_result <- anova(modelI)
ss_total <- sum(anova_result$`Sum Sq`)
relative_contrib <- anova_result$`Sum Sq`[-nrow(anova_result)] / 
                    sum(anova_result$`Sum Sq`[-nrow(anova_result)])
names(relative_contrib) <- rownames(anova_result)[-nrow(anova_result)]

cat("\nRelative contributions:\n")
print(sort(relative_contrib, decreasing = TRUE))
```

**1. Area - DOMINANT PREDICTOR (68.21%)**

-   **Contributes over two-thirds of the model's explanatory power**

-   The single most important factor in determining apartment prices

-   This makes intuitive sense: larger apartments command higher prices

-   Despite multicollinearity concerns, area's contribution is clearly dominant

**2. Zone - SECOND MAJOR PREDICTOR (20.80%)**

-   **Contributes about one-fifth of the explanatory power**

-   Location (zone) is the second most important factor

-   Confirms the real estate adage: "location, location, location"

-   Combined with area, these two variables explain **89.01%** of the total explained variance

**3. Category - MODERATE PREDICTOR (4.33%)**

-   Quality classification of the apartment

-   Contributes modestly to price prediction

-   Much less important than area and zone

**4. Structural Features - MINOR PREDICTORS (Combined: 4.81%)**

**Individual contributions:**

-   **Elevator:** 1.72%

-   **Toilets:** 1.59%

-   **Garage:** 1.50%

These amenities add value but are relatively minor compared to area and location.

**5. Environmental/Quality Features - SMALL PREDICTORS (Combined: 1.85%)**

**Individual contributions:**

-   **Out (exterior quality):** 0.74%

-   **Streetcategory:** 0.47%

-   **Heating:** 0.40%

-   **Storage:** 0.24%

These features have minimal individual impact on explaining price variability.

#### (viii) What is the variable that explains the most variability in Model (I)?

```{r}
cat("\nVariable explaining most variability:", 
    names(which.max(relative_contrib)), "\n")
```

#### (ix) What variables jointly explain 80% of the total variability of `log(totalprice)`?

```{r}
cumulative_var <- cumsum(sort(relative_contrib, decreasing = TRUE))
vars_80_idx <- which(cumulative_var <= 0.80)
cat("\nVariables jointly explaining approximately 80% of variability:\n")
print(names(sort(relative_contrib, decreasing = TRUE))[vars_80_idx])
```

#### (x) Find the predictions of Model (I) with bias correction and without bias correction. The bias correction is obtained by means of the lognormal distribution: If $\hat{Y}_{pred}$ is the prediction of Model (I), the corrected (backtransformed) prediction $\tilde{Y}_{pred}$ of Model (I) is given by

$$
\tilde{Y}_{pred} = \exp(\hat{Y}_{pred}+\hat{\sigma}^2/2)
$$

where $\hat{\sigma}^2$ is the variance of the error term, and the confidence interval is given by

$$
l_{inf} = \exp (\hat{Y}_{pred}+\hat{\sigma}/2-z_{1-\alpha/2}\sqrt{\widehat{Var}(\hat{Y}_{pred})+\widehat{Var}(\hat{\sigma}^2/4)} )
$$

$$
l_{sup} = \exp (\hat{Y}_{pred}+\hat{\sigma}/2+z_{1-\alpha/2}\sqrt{\widehat{Var}(\hat{Y}_{pred})+\widehat{Var}(\hat{\sigma}^2/4)} )
$$

and

$$
\widehat{Var}(\hat{\sigma})=\frac{2\hat{\sigma}^4}{df_{residual}}
$$

```{r}
sigma_sq <- summary(modelI)$sigma^2
df_res <- df.residual(modelI)

cat("Residual variance (σ²):", sigma_sq, "\n")
cat("Bias correction factor: exp(σ²/2) =", exp(sigma_sq/2), "\n")
```

```{r}
# Bias correction factor
bias_correction <- exp(sigma_sq/2)
# = 1.002852

# Get predictions on log scale
log_predictions <- predict(modelI, newdata = VIT2005_clean)

# WITHOUT bias correction (naive)
predictions_naive <- exp(log_predictions)

# WITH bias correction
predictions_corrected <- exp(log_predictions + sigma_sq/2)
# Or equivalently: 
predictions_corrected <- predictions_naive * bias_correction

# Compare
comparison <- data.frame(
  log_pred = log_predictions,
  naive = predictions_naive,
  corrected = predictions_corrected,
  difference = predictions_corrected - predictions_naive,
  pct_difference = (predictions_corrected - predictions_naive) / predictions_naive * 100
)
```

#### (xi) For Model (I),plot the predicted values (with and without bias correction) versus observed values. Comment on the results.

```{r}
# (xi) Plot predicted vs observed

# Get fitted values on log scale from Model I
fitted_log <- fitted(modelI)

# Get observed values - MUST match the cleaned dataset (without obs 3 and 93)
observed <- VIT2005_clean$totalprice  # This should already be correct

# Create side-by-side plots
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 3, 1))

# Plot 1: WITHOUT bias correction
plot(observed, predictions_naive, 
     main = "Predicted vs Observed\n(No Bias Correction)",
     xlab = "Observed Total Price (€)",
     ylab = "Predicted Total Price (€)",
     pch = 19, 
     col = rgb(70/255, 130/255, 180/255, 0.6),  # steelblue with transparency
     cex = 0.8)
abline(0, 1, col = "red", lwd = 2, lty = 2)  # 45-degree reference line
grid()

# Add correlation
cor_naive <- cor(observed, predictions_naive)
text(x = min(observed) + 0.1 * diff(range(observed)), 
     y = max(predictions_naive) - 0.05 * diff(range(predictions_naive)),
     labels = paste0("r = ", round(cor_naive, 4)),
     pos = 4, cex = 1.2)

# Plot 2: WITH bias correction
plot(observed, predictions_corrected, 
     main = "Predicted vs Observed\n(With Bias Correction)",
     xlab = "Observed Total Price (€)",
     ylab = "Predicted Total Price (€)",
     pch = 19, 
     col = rgb(34/255, 139/255, 34/255, 0.6),  # forestgreen with transparency
     cex = 0.8)
abline(0, 1, col = "red", lwd = 2, lty = 2)  # 45-degree reference line
grid()

# Add correlation
cor_corrected <- cor(observed, predictions_corrected)
text(x = min(observed) + 0.1 * diff(range(observed)), 
     y = max(predictions_corrected) - 0.05 * diff(range(predictions_corrected)),
     labels = paste0("r = ", round(cor_corrected, 4)),
     pos = 4, cex = 1.2)

# Reset graphics parameters
par(mfrow = c(1, 1))

# Calculate prediction accuracy metrics
cat("\n=== Prediction Accuracy Metrics ===\n")
cat("\nWithout Bias Correction:\n")
cat("  Correlation:", round(cor_naive, 4), "\n")
cat("  RMSE:", round(sqrt(mean((observed - predictions_naive)^2)), 2), "€\n")
cat("  MAE:", round(mean(abs(observed - predictions_naive)), 2), "€\n")
cat("  Mean Prediction:", round(mean(predictions_naive), 2), "€\n")
cat("  Mean Observed:", round(mean(observed), 2), "€\n")
cat("  Bias:", round(mean(predictions_naive - observed), 2), "€\n")

cat("\nWith Bias Correction:\n")
cat("  Correlation:", round(cor_corrected, 4), "\n")
cat("  RMSE:", round(sqrt(mean((observed - predictions_corrected)^2)), 2), "€\n")
cat("  MAE:", round(mean(abs(observed - predictions_corrected)), 2), "€\n")
cat("  Mean Prediction:", round(mean(predictions_corrected), 2), "€\n")
cat("  Mean Observed:", round(mean(observed), 2), "€\n")
cat("  Bias:", round(mean(predictions_corrected - observed), 2), "€\n")
```

**1. OVERALL MODEL PERFORMANCE - EXCELLENT**

**Strong Predictive Accuracy:**

-   **Correlation r = 0.9643** indicates very strong linear relationship

-   **R² = 0.9643² = 93.0%** of variance in observed prices is captured by predictions

-   Points cluster **tightly around the 45-degree line** throughout the entire price range

-   Model performs consistently well from low-priced (€150K) to high-priced (€550K+) apartments

**2. COMPARISON: WITH vs. WITHOUT BIAS CORRECTION**

**LEFT PLOT - Without Bias Correction (Blue):**

**Systematic Underestimation:**

-   **Mean predicted:** €280,558

-   **Mean observed:** €281,142

-   **Bias:** -€584 (negative = systematic underestimation)

-   Points show a **slight tendency to fall below the 45-degree line**, particularly noticeable when comparing means

-   The model systematically underpredicts by approximately **0.21%**

**Accuracy Metrics:**

-   RMSE: €18,363

-   MAE: €14,429

-   Typical prediction error: \~€14,400-€18,400

**RIGHT PLOT - With Bias Correction (Green):**

**Improved Centering:**

-   **Mean predicted:** €281,358

-   **Mean observed:** €281,142

-   **Bias:** +€216 (nearly eliminated, reduced by 63%)

-   Points are **better centered around the 45-degree line**

-   Slight overcorrection (now +€216 instead of -€584)

**Accuracy Metrics:**

-   RMSE: €18,350 (slightly better, ↓ €13)

-   MAE: €14,441 (marginally higher, ↑ €12)

-   Overall bias **dramatically reduced** from -€584 to +€216

**Net Effect of Bias Correction:**

-   **Bias reduction:** 63% improvement (from -€584 to +€216)

-   **More balanced predictions:** Residual bias now only +0.08% vs. -0.21%

-   **Marginal trade-off:** Slightly lower RMSE but marginally higher MAE

**3. PATTERN ANALYSIS**

**Linearity:**

-   Excellent linear relationship across the entire price range

-   No evidence of non-linear patterns or systematic curvature

-   Model assumptions appear well-satisfied

**Homoscedasticity:**

-   Vertical scatter around the 45-degree line appears **relatively constant** across price levels

-   No obvious funnel shape (increasing or decreasing variance)

-   Consistent prediction accuracy from low to high prices

**Distribution of Residuals:**

-   Points scatter **symmetrically** around the reference line

-   Most predictions within ±€20,000-€30,000 of observed values

-   A few outliers with larger errors, but these are rare

**4. OUTLIERS AND EXTREME PREDICTIONS**

**Well-Predicted Range:**

-   The majority of apartments (€200K-€450K) show excellent prediction accuracy

-   Tight clustering around the diagonal

**Potential Outliers:**

-   A few observations show larger deviations:

    -   Some high-priced apartments (\>€500K) are slightly underpredicted

    -   A few mid-range apartments show larger positive/negative residuals

-   Overall, outliers are **minimal and non-systematic**

**5. MODEL VALIDATION**

**Evidence of Good Model:**

-   High correlation (0.9643)

-   Low systematic bias after correction (+€216, or 0.08%)

-   Consistent performance across price ranges

-   No obvious violations of assumptions

-   Reasonable prediction errors (MAE ≈ 5% of mean price)

**Comparison to Model Statistics:**

-   In-sample R² = 93.17% (from regression output)

-   Correlation² = 0.9643² = 93.0% (from plot)

-   **Excellent agreement** between model fit statistics and actual predictions

#### (xii) Show that in Model (I) an increment of $10 m^2$ in the area of a flat implies an increment of roughly 4% in the predicted total price. To verify this, find the predicted price of three apartments with areas of 80, 90, and $100m^2$, respectively, and keep the rest of the explanatory variables fixed. For example, assign the following values to the explanatory variables: `zone = Z32`, `elevator = 1`, `toilets = 1`, `garage = 1`, `category = 3B`, `out= E50`, `storage = 1`, `heating = 3A`, and `streetcategory = S3`. Compute the corresponding 90% prediction intervals.

```{r}
# Create prediction data for apartments with 80, 90, 100 m²
# Note: Adjust factor levels based on actual data
pred_data <- data.frame(
  area = c(80, 90, 100),
  zone = factor(rep("Z32", 3)),
  elevator = rep(1, 3),
  toilets = rep(1, 3),
  garage = rep(1, 3),
  category = factor(rep("3B", 3)),
  out = factor(rep("E50", 3)),
  storage = rep(1, 3),
  heating = factor(rep("3A", 3)),
  streetcategory = factor(rep("S3", 3))
)

# Make predictions
preds <- predict(modelI, newdata = pred_data, 
                 interval = "prediction", level = 0.90)

# Back-transform with bias correction
predicted_prices <- exp(preds[, "fit"] + sigma_sq/2)
lower_90 <- exp(preds[, "lwr"] + sigma_sq/2)
upper_90 <- exp(preds[, "upr"] + sigma_sq/2)

cat("\nPredicted prices for 80, 90, 100 m²:\n")
for(i in 1:3) {
  cat(sprintf("Area %d m²: €%.2f [90%% PI: €%.2f, €%.2f]\n",
              pred_data$area[i], predicted_prices[i], 
              lower_90[i], upper_90[i]))
}

# Calculate percentage changes
pct_change_80_90 <- (predicted_prices[2] - predicted_prices[1]) / 
                     predicted_prices[1] * 100
pct_change_90_100 <- (predicted_prices[3] - predicted_prices[2]) / 
                      predicted_prices[2] * 100

cat(sprintf("\nPercentage change (80 to 90 m²): %.2f%%\n", pct_change_80_90))
cat(sprintf("Percentage change (90 to 100 m²): %.2f%%\n", pct_change_90_100))
cat(sprintf("Average percentage change per 10 m²: ~%.2f%%\n", 
            mean(c(pct_change_80_90, pct_change_90_100))))
```

#### (xiii) What is the percentage change in the total price of an apartment when the number of garages changes from one to two?

```{r}
if("garage" %in% names(coef(modelI))) {
  garage_coef <- coef(modelI)["garage"]
  pct_change_garage <- (exp(garage_coef) - 1) * 100
  cat(sprintf("Percentage change when garage increases by 1: %.2f%%\n", 
              pct_change_garage))
} else {
  cat("Garage variable not in final model.\n")
}
```

#### (xiv) What is the percentage change in the total price of an apartment when the heating type changes from “1A” to “3B”?

```{r}
heating_coefs <- coef(modelI)[grep("heating", names(coef(modelI)))]
if(length(heating_coefs) > 0) {
  cat("Heating coefficients in model:\n")
  print(heating_coefs)
  cat("\nNote: Percentage change depends on reference category.\n")
  cat("Calculate as: exp(coef_3B - coef_1A) - 1 if both present,\n")
  cat("or exp(coef) - 1 for change from reference to specified level.\n")
} else {
  cat("Heating variable not in final model.\n")
}
```
